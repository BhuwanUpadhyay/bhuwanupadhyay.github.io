{"pages":[{"title":"Bhuwan Prasad Upadhyay","text":"Enthusiastic software engineer with 7+ years of experience participating in the complete product development lifecycle of successfully launched applications. Hands-on professional experience in Java, Kotlin, Spring, Hibernate/JPA, Angular, Blockchain, AWS, Azure, Microservices, Kubernetes. In previous roles, migrated monolithic applications into microservices, developed greenfield products using cloud-native and microservice architecture, refactored code base to increase maintainability, and resolved a performance bottleneck that increased application performance. Always determined to make substantial impacts in the future. Have basic knowledge on ML and keen to explore in depth. Java, Kotlin, Angular, Spring Boot, Spring Cloud, Blockchain, AWS, Azure, Microservices, Domain Driven Design, Cloud Native Application, Software Design, Coding Problems Are you interested to read more about my experiences?? document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/about/index.html"}],"posts":[{"title":"Serverless AWS Lambda function with dynamo db","text":"This article demonstrates, how we can use serverless framework to deploy lambda function with dynamo DB. We will create project from scratch and deploy to aws using serverless framework. Code ExampleThis article is accompanied by working example code on GitHub. Create Handler FunctionsLet’s consider we have to expose APIs to manage an order: Put order into the db (DynamoDB) Retrieve orders from the db (DynamoDB). To achieve these two operations in serverless, we need two lambda functions with http event type: POST : Store order into the db GET : Fetch orders to the client AWS Lambda Proxy Integration I/O format Before write a code, let’s understand input and output format supported by aws lambda proxy integration in API gateway. You can check on following links: Input Format Output Format According to AWS documentation, java class for input format is: 12345678910111213141516171819@Datapublic class ApiGatewayRequest { private String resource; private String path; private String httpMethod; private Map&lt;String, String&gt; headers; private Map&lt;String, String&gt; queryStringParameters; private Map&lt;String, String&gt; pathParameters; private Map&lt;String, String&gt; stageVariables; private Map&lt;String, Object&gt; requestContext; private String body; private boolean isBase64Encoded; @SneakyThrows &lt;T&gt; T toBody(Class&lt;T&gt; valueType) { return MAPPER.readValue(body, valueType); }} and java class for out format with its builder class to simply construction object simple: 12345678910111213141516171819202122232425262728293031323334353637383940@Getterpublic class ApiGatewayResponse { public static final ObjectMapper MAPPER = new ObjectMapper(); private final int statusCode; private final String body; private final Map&lt;String, String&gt; headers = new HashMap&lt;&gt;(); private final boolean isBase64Encoded; private ApiGatewayResponse(int statusCode, String body) { this.statusCode = statusCode; this.body = body; this.setHeaders(); this.isBase64Encoded = false; } public static ApiGatewayResponse bad(String message) { return build(HttpStatus.SC_BAD_REQUEST, message); } public static &lt;T&gt; ApiGatewayResponse ok(T body) { return build(HttpStatus.SC_OK, body); } static ApiGatewayResponse serverError(String message) { return build(HttpStatus.SC_INTERNAL_SERVER_ERROR, message); } @SneakyThrows private static &lt;T&gt; ApiGatewayResponse build(int statusCode, T body) { return new ApiGatewayResponse(statusCode, MAPPER.writeValueAsString(body)); } private void setHeaders() { headers.put(\"X-Powered-By\", \"BhuwanUpadhyay\"); headers.put(\"Content-Type\", \"application/json\"); headers.put(\"Access-Control-Allow-Origin\", \"*\"); }} AWS Dynamo DB PersistenceTo persist customer orders into the dynamo db we need to define data object which match with our table structure in dynamo. 123456789@Data@DynamoDBTable(tableName = \"Order\")public class Order { @DynamoDBHashKey private String orderId; private String description; private String customer;} Now, we need a repository to interact with dynamo db. 12345678910111213141516171819202122public class OrderRepository { private final DynamoDBMapper mapper; public OrderRepository() { mapper = new DynamoDBMapper(AmazonDynamoDBClientBuilder.standard().build()); } public List&lt;Order&gt; getOrders() { PaginatedScanList&lt;Order&gt; orders = mapper.scan(Order.class, new DynamoDBScanExpression()); return new ArrayList&lt;&gt;(orders); } public Order save(Order order) { mapper.save(order); return mapper.load(order); } public Optional&lt;Order&gt; findByOrderId(String orderId) { return Optional.ofNullable(mapper.load(Order.class, orderId)); }} AWS Serverless Lambda FunctionNow, before create lambda function i will create one abstract class which encapsulate common behaviour of all lambda function which are going to create on this demo. 1234567891011121314151617181920212223242526272829public abstract class HttpEventHandler&lt;T&gt; implements RequestHandler&lt;ApiGatewayRequest, ApiGatewayResponse&gt; { @Override public ApiGatewayResponse handleRequest(ApiGatewayRequest input, Context context) { try { return this.handle(toRequestIfNoVoidType(input), context.getLogger()); } catch (Exception e) { return ApiGatewayResponse.serverError(ExceptionUtils.getStackTrace(e)); } } protected abstract ApiGatewayResponse handle(T request, LambdaLogger log); private T toRequestIfNoVoidType(ApiGatewayRequest input) { final Class&lt;T&gt; bodyType = getBodyType(); return !bodyType.equals(Void.class) ? input.toBody(bodyType) : null; } @SuppressWarnings(\"unchecked\") private Class&lt;T&gt; getBodyType() { try { String typeName = ((ParameterizedType) getClass().getGenericSuperclass()).getActualTypeArguments()[0].getTypeName(); return (Class&lt;T&gt;) Class.forName(typeName); } catch (Exception e) { throw new RuntimeException(\"Class is not parametrized with generic type!!! Please use extends &lt;&gt; \", e); } }} Now, time come to create our serverless lambda function. OrderProcessHandler 12345678910111213public class OrderProcessHandler extends HttpEventHandler&lt;Order&gt; { private final OrderRepository repository = new OrderRepository(); @Override protected ApiGatewayResponse handle(Order request, LambdaLogger log) { Optional&lt;Order&gt; order = repository.findByOrderId(request.getOrderId()); return order. map(o -&gt; bad(String.format(\"Order already exist with id %s\", o.getOrderId()))). orElse(ok(repository.save(request))); }} FetchOrderHandler12345678910public class FetchOrderHandler extends HttpEventHandler&lt;Void&gt; { private final OrderRepository repository = new OrderRepository(); @Override protected ApiGatewayResponse handle(Void request, LambdaLogger log) { log.log(\"Fetching orders....\"); return ApiGatewayResponse.ok(repository.getOrders()); }} We have done with coding so far. Next step is launch our function into aws cloud platform and evaluate the result. Serverless FrameworkSeverless framework simply deployment process for lambda function. To use it we need to create serverless.yml file in our project and configure our lambda function and DynamoDB tables as follows: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647service: order-serviceprovider: name: aws runtime: java8 stage: ${opt:stage, 'dev'} iamRoleStatements: - Effect: \"Allow\" Resource: \"*\" Action: - \"dynamodb:*\"package: artifact: target/app-jar-with-dependencies.jarfunctions: order-process: handler: bhuwanupadhyay.serverless.order.OrderProcessHandler events: - http: path: orders method: post cors: true fetch-order: handler: bhuwanupadhyay.serverless.order.FetchOrderHandler events: - http: path: orders method: get cors: trueresources: Resources: Order: Type: AWS::DynamoDB::Table Properties: TableName: Order AttributeDefinitions: - AttributeName: orderId AttributeType: S KeySchema: - AttributeName: orderId KeyType: HASH ProvisionedThroughput: ReadCapacityUnits: 1 WriteCapacityUnits: 1 make buildWhen you run the command make run then it will create uber jar with its all needed dependencies at runtime. For this you have configured maven-assembly-plugin as below: 12345678910111213141516171819&lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;!-- get all project dependencies --&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;!-- bind to the packaging phase --&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; make deployWhen you run the command make deploy then it will deploy the lambda function to AWS and perform tests to verify those deployed functions. document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/08/serverless-aws-lambda-function-with-dynamo-db/"},{"title":"BigDecimal to String in java","text":"In Java, there are two very simple approaches to convert BigDecimal to String without stripping trailing zeros. Using BigDecimal scale Using DecimalFormat Example12345678910111213141516171819202122232425public class BigDecimalToStringTest { @Test public void usingScale() { assertEquals(\"1000.000\", numberToStringWithTrailZeros(1000.000, 3)); assertEquals(\"1000.10\", numberToStringWithTrailZeros(1000.10, 2)); } @Test public void usingDecimalFormat() { DecimalFormat _3DigitDecimalFormat = new DecimalFormat(\"#0.000\"); DecimalFormat _2DigitDecimalFormat = new DecimalFormat(\"#0.00\"); assertEquals(\"1000.000\", _3DigitDecimalFormat.format(new BigDecimal(1000.000))); assertEquals(\"1000.123\", _3DigitDecimalFormat.format(new BigDecimal(1000.1234))); assertEquals(\"1000.10\", _2DigitDecimalFormat.format(new BigDecimal(1000.10000))); } private String numberToStringWithTrailZeros(double number, int scaleDigits) { BigDecimal decimal = BigDecimal.valueOf(number); return decimal.setScale(scaleDigits, RoundingMode.HALF_UP).toPlainString(); }} document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/08/bigdecimal-to-string-in-java/"},{"title":"AWS Lambda load testing with large scale data using Gatling","text":"Load testing is a type of non-functional testing. A load test is type of software testing which is conducted to understand the behavior of the application under a specific expected load. Load testing is performed to determine a system’s behavior under both normal and at peak conditions. No amount of testing can prove a software right, a single test can prove a software wrong. - Amir Ghahrai Example ScenarioLet’s say, We have to perform Two Million HTTP requests load in AWS Lambda function via API Gateway. Obviously, to perform 2M requests we need to run load testing in cluster mode. Gatling Frontline support cluster load testing directly. With Gatling Community Version we need to do some extra work to achieve cluster mode load testing. Code ExampleThis article is accompanied by working example code on GitHub Create Handler Functions - on GithubLet’s consider we have to expose APIs to manage an order: Put order into the db (DynamoDB) To achieve these two operations in serverless, we need two lambda functions with http event type: POST : Store order into the db Load Testing - on GithubI used gatling-aws-maven-plugin to run 20 EC2 instances together, which runs load testing parallel. For two millions http requests by using 20 instances together, in gatling our plan will be like below: 12345REPETITION -&gt; 2000 NO_OF_USERS -&gt; 50-----------------------------------------------TOTAL REQUEST = 2000 * 50 = 100000WITH 20 INSTANCES = 100000 * 20 = 2000000 = 2M Now our load testing simulation will looks like: 12345678910111213141516171819202122232425262728293031323334import java.util.UUID.randomUUIDimport io.gatling.core.Predef._import io.gatling.core.structure.ScenarioBuilderimport io.gatling.http.Predef._class LoadTestingSimulation extends Simulation { private val ENDPOINT = \"https://xxxxx.execute-api.ap-southeast-2.amazonaws.com/test\" private val BODY: String = \"{\\\"customerId\\\" : \\\"@@customerId@@\\\" }\" private val scn: ScenarioBuilder = scenario(\"Load Testing\") .repeat(2000) { exec( http(\"initiation\") .post(ENDPOINT + \"/orders\") .check(status.is(200)) .body(StringBody(session =&gt; BODY.replaceAll(\"@@customerId@@\", newId))) .header(\"Content-type\", \"text/xml\") ) } setUp(scn.inject(atOnceUsers(50)) .protocols(http .baseURL(ENDPOINT) .acceptHeader(\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\") .acceptEncodingHeader(\"gzip, deflate\") .acceptLanguageHeader(\"en-US,en;q=0.5\") .userAgentHeader(\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:16.0) Gecko/20100101 Firefox/16.0\"))) private def newId = { randomUUID().toString.substring(0, 33) }} make deployWhen you run the command make deploy then it will deploy the lambda function to AWS. make testTo run the command make test provide correct AWS accessKey and secretKey in aws.properties. Also, change values in runLoadtestRemotely.sh according to you configuration. 1234567S3_BUCKET=load-testing-s3-bucketEC2_KEY_PAIR=loadtest-keypairNO_OF_INSTANCES=20EC2_ENDPOINT=https://ec2.us-west-1.amazonaws.comEC2_SECURITY_GROUP_ID=sg-xxxxxxxAMI_ID=ami-0782017a917e973e7SSH_USER=ec2-user Then you can run make test and analyze the result. document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/09/aws-lambda-load-testing-with-large-scale-data-using-gatling/"},{"title":"Log aggregation for AWS Lambda","text":"ELK is the acronym for three open source projects: Elasticsearch, Logstash, and Kibana. Elasticsearch is a search and analytics engine. Logstash is a server‑side data processing pipeline that ingests data from multiple sources simultaneously, transforms it, and then sends it to a “stash” like Elasticsearch. Kibana lets users visualize data with charts and graphs in Elasticsearch. This article demonstrates, how we can use ELK Stack with lambda function for log aggregation. I get paid for produce code that works, not for writing tests. - Kent Back Example ScenarioIn the post, I explained an approach of using a Lambda function and Kinesis Stream to ship all your lambda logs from CloudWatch logs to a log aggregation service such as ELK Stack-(Elasticsearch, Logstash, Kibana). Create Handler FunctionsLet’s consider we have to expose APIs to manage an order with ELK: Put order into the db (DynamoDB) Retrieve orders from the db (DynamoDB). Send CloudWatch logs from Kinesis Stream to ELK To achieve these two operations in serverless, we need three lambda functions: POST : Store order into the db - on Github GET : Fetch orders to the client - on Github KinesisLogsStream : Ship logs to ELK - on Github Serverless FrameworkSeverless framework simply deployment process for lambda function. To use it we need to create serverless.yml file in our project and configure our lambda function , DynamoDB, Kinesis Stream as follows: Note: logstash_host, logstash_port and token configure with your ELK environment. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144service: name: order-apisplugins:- serverless-pseudo-parameters- serverless-iam-roles-per-functioncustom: region: ${opt:region, self:provider.region} stage: ${opt:stage} prefix: ${self:service}-${self:custom.stage} dynamodb_table: ${self:custom.prefix}-order dynamodb_arn: arn:aws:dynamodb:${self:custom.region}:*:table/${self:custom.dynamodb_table}provider: name: aws runtime: nodejs8.10 region: ${opt:region, 'us-east-1'} timeout: 30 iamRoleStatements: - Effect: Allow Action: - dynamodb:Query - dynamodb:Scan - dynamodb:GetItem - dynamodb:PutItem - dynamodb:UpdateItem - dynamodb:DeleteItem - dynamodb:DescribeTable Resource: ${self:custom.dynamodb_arn} environment: DYNAMO_TABLE: ${self:custom.dynamodb_table}package: exclude: - .idea/** - .git/** - tmp/**functions: create-order: handler: functions/create-order/handler.handle description: Create new order events: - http: path: orders method: post get-orders: handler: functions/get-orders/handler.handle description: Get orders events: - http: path: orders method: get ship-logs-to-elk: handler: functions/ship-logs-to-elk/handler.handle description: Sends CloudWatch logs from Kinesis to ELK events: - stream: type: kinesis arn: Fn::GetAtt: - KinesisLogsStream - Arn environment: logstash_host: listener.logz.io #&lt;INSERT VALUE HERE&gt; logstash_port: 5050 #&lt;INSERT VALUE HERE&gt; token: &lt;INSERT VALUE HERE&gt;resources: Resources: Order: Type: AWS::DynamoDB::Table Properties: TableName: ${self:custom.dynamodb_table} AttributeDefinitions: - AttributeName: orderId AttributeType: S KeySchema: - AttributeName: orderId KeyType: HASH ProvisionedThroughput: ReadCapacityUnits: 1 WriteCapacityUnits: 1 KinesisLogsStream: Type: AWS::Kinesis::Stream Properties: Name: ${self:custom.prefix}-logs ShardCount: 1 CWLtoKinesisRole: Type: AWS::IAM::Role Properties: RoleName: ${self:custom.prefix}-CWLtoKinesisRole AssumeRolePolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Principal: Service: logs.${self:custom.region}.amazonaws.com Action: sts:AssumeRole PermissionsPolicyForCWL: Type: AWS::IAM::Policy DependsOn: - CWLtoKinesisRole - KinesisLogsStream Properties: PolicyName: ${self:custom.prefix}-PermissionsPolicyForCWL Roles: - Ref: CWLtoKinesisRole PolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Action: kinesis:PutRecord Resource: Fn::GetAtt: - KinesisLogsStream - Arn - Effect: Allow Action: iam:PassRole Resource: Fn::GetAtt: - CWLtoKinesisRole - Arn CWLtoKinesisSubscription: Type: AWS::Logs::SubscriptionFilter DependsOn: - CWLtoKinesisRole - PermissionsPolicyForCWL - KinesisLogsStream Properties: DestinationArn: Fn::GetAtt: - KinesisLogsStream - Arn RoleArn: Fn::GetAtt: - CWLtoKinesisRole - Arn FilterPattern: \"\" LogGroupName: /aws/lambda/${self:custom.prefix}-create-order make deployWhen you run the command make deploy then it will deploy the lambda function to AWS and perform test on lambda function and cloud watch logs will be shipped to the ELK. document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/09/log-aggregation-for-aws-lambda/"},{"title":"Enable Elastic APM in Spring Boot Application","text":"Elastic APM instruments the applications to ship performance metrics to Elasticsearch for visualization in Kibana with pre-configured dashboards. This article demonstrates, how we can use Elastic Apm with spring boot application for monitoring. In business, the idea of measuring what you are doing, picking the measurements that count like customer satisfaction and performance… you thrive on that. - Bill Gates Example ScenarioFirst of all, We need to create one spring boot project with JPA support, to do so I will use a spring command line which will create a skeleton project for us. The following command we need to run for the setup of the project: 123456789spring init\\ -n=order-service\\ -d=web,jpa,h2,lombok\\ --groupId=io.github.bhuwanupadhyay\\ --package-name=io.github.bhuwanupadhyay.order\\ --description=\"Order Service\"\\ --version=1.0-SNAPSHOT\\ --language=java \\order-service Note: if you don’t have spring command line tool yet please refer. Now, Let’s create simple CRUD operation REST endpoints for order domain. 123456789101112131415161718192021222324252627282930313233@Entity@Table(name = \"app_orders\")@Getterclass Order { @Id private String orderId; private String itemId; private String customerId;}interface OrderRepository extends CrudRepository&lt;Order, String&gt; {}@RestController@RequestMapping(\"/orders\")@RequiredArgsConstructorclass OrderEndpoints { private final OrderRepository repository; @GetMapping public ResponseEntity&lt;List&lt;Order&gt;&gt; listOrders() { return ResponseEntity.ok((List&lt;Order&gt;) repository.findAll()); } @PostMapping public ResponseEntity&lt;Order&gt; createOrder(@RequestBody Order order) { return ResponseEntity.ok(repository.save(order)); }} The Dockerfile for order-service application to run elastic-apm-agent.jar 1234567891011121314FROM openjdk:8-jdk-alpineVOLUME /tmpARG JAR_FILEARG ELASTIC_APM_AGEN_VERSION=0.7.0RUN wget https://search.maven.org/remotecontent?filepath=co/elastic/apm/elastic-apm-agent/${ELASTIC_APM_AGEN_VERSION}/elastic-apm-agent-${ELASTIC_APM_AGEN_VERSION}.jar \\ -O ${HOME}/elastic-apm-agent.jarCOPY ${JAR_FILE} app.jarENTRYPOINT [ \"java\", \"-javaagent:${HOME}/elastic-apm-agent.jar\", \"-Djava.security.egd=file:/dev/./urandom\", \"-jar\", \"/app.jar\" ] The complete docker-compose.yml file which includes all necessary components: Elasticsearch Kibana Elastic APM Server Order Service [ Spring Boot Application ] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647version: \"3\"services: apm-elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:6.4.2 ports: - 9200 kibana: image: docker.elastic.co/kibana/kibana:6.4.2 environment: ELASTICSEARCH_URL: http://apm-elasticsearch:9200 ports: - 5601:5601 links: - apm-elasticsearch depends_on: - apm-elasticsearch elastic-apm-server: image: docker.elastic.co/apm/apm-server:6.4.2 command: [ \"bash\", \"-c\", \"apm-server run -e -E output.elasticsearch.hosts=['http://apm-elasticsearch:9200'] \"] ports: - 8200 links: - apm-elasticsearch depends_on: - apm-elasticsearch order-service: image: docker.io/devbhuwan/order-service build: context: . dockerfile: Dockerfile args: - JAR_FILE=target/order-service-1.0-SNAPSHOT.jar ports: - 8080:8080 environment: ELASTIC_APM_SERVICE_NAME: order-service ELASTIC_APM_APPLICATION_PACKAGES: io.github.bhuwanupadhyay ELASTIC_APM_SERVER_URLS: http://elastic-apm-server:8200 links: - elastic-apm-server depends_on: - elastic-apm-server The following command to launch service with elastic apm agent and apm server components: 1mvn clean package &amp;&amp; docker-compose up Finally, we need to perform load testing to see some result in kibana dashboard. To do so i will use load testing tool artillery. 123mkdir artillery-load-testingcd artillery-load-testingtouch package.json script.yml processor.js package.json 1234567891011121314{ \"name\": \"artillery-load-testing\", \"version\": \"1.0.0\", \"description\": \"Artillery Load Testing\", \"main\": \"index.js\", \"scripts\": { \"test\": \"./node_modules/.bin/artillery run script.yml\" }, \"license\": \"MIT\", \"dependencies\": { \"artillery\": \"^1.6.0-24\", \"uuid\": \"^3.3.2\" }} script.yml 1234567891011121314151617181920config: target: \"http://localhost:8080\" processor: \"./processor.js\" phases: - duration: 60 arrivalRate: 20scenarios:- name: \"POST\" flow: - post: headers: content-type: \"application/json\" url: \"/orders\" beforeRequest: \"assignNewId\" body: | { \"orderId\": \"@@id@@\", \"itemId\": \"1\", \"customerId\": \"1\" } processor.js 123456789101112'use strict';const uuidv1 = require('uuid/v1');// noinspection JSUnresolvedVariablemodule.exports = {assignNewId};function assignNewId(requestParams, context, eventEmitter, done) { const id = uuidv1(); requestParams.body = requestParams.body .replace(\"@@id@@\", id); return done();} The following command to perform load testing: 12npm installnpm run test Now, we need to analyze performance metrics in kibana dashboard under APM menu link in the left sidebar. document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/10/enable-elastic-apm-in-spring-boot-application/"},{"title":"Performance Benchmarking with Tomcat","text":"This article demonstrates, how we can use Elastic Apm with tomcat web application for monitoring. We will create project from scratch and run with elastic apm. Just as athletes can’t win without a sophisticated mixture of strategy, form, attitude, tactics, and speed, performance engineering requires a good collection of metrics and tools to deliver the desired business results. - Todd DeCapua On this article, I am going to explain how to use Elastic APM with tomcat. The Dockerfile for application to run elastic-apm-agent.jar with tomcat. 123456789101112131415FROM tomcat:8.5.34-jre8-alpineARG WAR_FILEARG ELASTIC_APM_AGEN_VERSION=0.7.0COPY ${WAR_FILE} /usr/local/tomcat/app.warRUN cd /usr/local/tomcat &amp;&amp; \\ rm -rf webapps/* &amp;&amp; \\ mkdir -p webapps/ROOT &amp;&amp; \\ unzip app.war -d webapps/ROOT &gt; /dev/null &amp;&amp; \\ rm /usr/local/tomcat/app.warRUN wget https://search.maven.org/remotecontent?filepath=co/elastic/apm/elastic-apm-agent/${ELASTIC_APM_AGEN_VERSION}/elastic-apm-agent-${ELASTIC_APM_AGEN_VERSION}.jar \\ -O ${HOME}/elastic-apm-agent.jarADD bin /usr/local/tomcat/binWORKDIR /usr/local/tomcat/binENV JPDA_ADDRESS 8000CMD [\"catalina.sh\", \"jpda\", \"run\"] 123mkdir bin &amp;&amp; cd bintouch setenv.shchmod +x setenv.sh setenv.sh 123456789101112#!/usr/bin/env bashexport CATALINA_OPTS=\" $CATALINA_OPTS -javaagent:${HOME}/elastic-apm-agent.jar -Delastic.apm.service_name=${ELASTIC_APM_SERVICE_NAME} -Delastic.apm.application_packages=${ELASTIC_APM_APPLICATION_PACKAGES} -Delastic.apm.server_urls=${ELASTIC_APM_SERVER_URLS}\"echo \"\"echo \"--------------------------------------------------------------\"echo \"CATALINA_OPTS: ${CATALINA_OPTS}\"echo \"--------------------------------------------------------------\"echo \"\" docker-compose.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546version: \"3\"services: apm-elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:6.4.2 ports: - 9200 kibana: image: docker.elastic.co/kibana/kibana:6.4.2 environment: ELASTICSEARCH_URL: http://apm-elasticsearch:9200 ports: - 5601:5601 links: - apm-elasticsearch elastic-apm: image: docker.elastic.co/apm/apm-server:6.4.2 command: [ \"bash\", \"-c\", \"apm-server run -e -E output.elasticsearch.hosts=['http://apm-elasticsearch:9200'] \"] links: - apm-elasticsearch ports: - 8200 performance-benchmarking-with-tomcat: image: docker.io/devbhuwan/performance-benchmarking-with-tomcat build: context: cicd/docker dockerfile: Dockerfile args: - WAR_FILE=Spring Boot-order-crud-service.war ports: - 8000:8000 - 8080:8080 environment: ELASTIC_APM_SERVICE_NAME: performance-benchmarking-with-tomcat ELASTIC_APM_APPLICATION_PACKAGES: io.github.bhuwanupadhyay ELASTIC_APM_SERVER_URLS: http://elastic-apm:8200 depends_on: - apm-elasticsearch - elastic-apm - kibana links: - elastic-apm Take a look at this repository Github to see how it works. document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/10/performance-benchmarking-with-tomcat/"},{"title":"How to create binary tree in Java","text":"Given a Binary tree, how to implement this binary tree in java? For example, for this Binary tree it has 8 nodes. Tree Node1234567891011121314151617181920212223242526272829303132333435363738394041/** * Tree Node Class */class Node { private Object value; private Node left; private Node right; public Node() { } public Node(Object value) { this.value = value; } public Object getValue() { return value; } public void setValue(Object value) { this.value = value; } public Node getLeft() { return left; } public void setLeft(Node left) { this.left = left; } public Node getRight() { return right; } public void setRight(Node right) { this.right = right; }} Binary Tree123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * Binary tree */class BinaryTree { private Node root; /** * Create new binary tree with root node * * @param root Root node */ public BinaryTree(Node root) { this.setRoot(root); } /** * Create new empty binary tree */ public BinaryTree() { this.setRoot(null); } public Node getRoot() { return root; } public void setRoot(Node root) { this.root = root; } /** * Insert new node into the tree. &lt;br&gt; * This method will insert new node into last tree level, until last tree level is full, then add new level. * * @param newNode new tree node */ public void insert(Node newNode) { if (this.root == null) { this.root = newNode; } else if (this.root.getLeft() == null) { this.root.setLeft(newNode); } else if (this.root.getRight() == null) { this.root.setRight(newNode); } else { List&lt;Node&gt; siblingNodes = new LinkedList&lt;&gt;(); siblingNodes.add(this.root.getLeft()); siblingNodes.add(this.root.getRight()); insertSiblings(siblingNodes, newNode); } } /** * Check a level sibling nodes, find the node which dones't have left or right child, then insert the new node * * @param siblingNodes List of current level tree nodes * @param newNode new tree node */ private void insertSiblings(List&lt;Node&gt; siblingNodes, Node newNode) { List&lt;Node&gt; nextSiblingNodes = new LinkedList&lt;&gt;(); for (Node currentNode : siblingNodes) { if (currentNode.getLeft() == null) { currentNode.setLeft(newNode); return; } else if (currentNode.getRight() == null) { currentNode.setRight(newNode); return; } nextSiblingNodes.add(currentNode.getLeft()); nextSiblingNodes.add(currentNode.getRight()); } insertSiblings(nextSiblingNodes, newNode); }} Test Scenario123456789101112131415161718192021222324252627282930313233343536373839404142434445public class BinaryTreeTest { @Test public void canCreateBinaryTree1() { BinaryTree tree = new BinaryTree(new Node(5)); tree.insert(new Node(4)); tree.insert(new Node(9)); tree.insert(new Node(3)); tree.insert(new Node(7)); tree.insert(new Node(8)); tree.insert(new Node(6)); tree.insert(new Node(5)); assertNotNull(tree.getRoot()); assertEquals(5, tree.getRoot().getValue()); assertEquals(4, tree.getRoot().getLeft().getValue()); assertEquals(9, tree.getRoot().getRight().getValue()); } @Test public void canCreateBinaryTree2() { BinaryTree tree = new BinaryTree(new Node(5)); tree.insert(new Node(4)); tree.insert(new Node(9)); tree.insert(new Node(3)); tree.insert(new Node(7)); Node node = new Node(8); node.setLeft(new Node(5)); tree.insert(node); tree.insert(new Node(6)); tree.insert(new Node(5)); assertNotNull(tree.getRoot()); assertEquals(1, tree.getRoot().getValue()); assertEquals(2, tree.getRoot().getLeft().getValue()); assertEquals(3, tree.getRoot().getRight().getValue()); } } document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/01/how-to-create-binary-tree-in-java/"},{"title":"Find the vertical sum of binary tree","text":"Given a Binary tree, how will you find the Vertical Sum of Binary Tree? For example, for this Binary tree it has 5 vertical lines. For line 3 the sum will be 5+7+8=20. Solution We need to check the horizontal distance (HD) from root for all nodes. HD for root is 0. For right child we will +1 (add 1) to HD For left child we will -1 (subtract 1) to HD We can easily maintain a hash map for horizontal distance corresponding to each vertical line. Then, we can traverse the Binary Tree and update our hash map. Algorithm - To find the vertical sum of binary tree1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Algorithm { /** * * @param tree binary tree * @param line vertical line number * @return sum of those nodes falls under that vertical line */ public int sumOfVerticalLine(BinaryTree tree, int line) { Map&lt;Integer, Integer&gt; sums = new LinkedHashMap&lt;&gt;(); Queue&lt;TraversalNode&gt; nodes = new LinkedList&lt;&gt;(); nodes.add(new TraversalNode(tree.getRoot(), 0)); while (!nodes.isEmpty()) { TraversalNode node = nodes.remove(); Integer value = (Integer) node.node.getValue(); sums.put(node.hd, sums.getOrDefault(node.hd, 0) + value); if (node.node.getLeft() != null) { nodes.add(new TraversalNode(node.node.getLeft(), node.hd - 1)); } if (node.node.getRight() != null) { nodes.add(new TraversalNode(node.node.getRight(), node.hd + 1)); } } List&lt;Integer&gt; hds = sums.keySet().stream().sorted().collect(Collectors.toList()); return sums.get(hds.get(line - 1)); } public static class TraversalNode { private Node node; /** * horizontal distance of node */ private int hd; public TraversalNode(Node node, int hd) { this.node = node; this.hd = hd; } }} Test Scenario123456789101112131415161718192021222324252627282930313233343536public class AlgorithmTest { @Test public void canEvaluateVerticalSumOfBinaryTree1() { BinaryTree tree = new BinaryTree(new Node(5)); tree.insert(new Node(4)); tree.insert(new Node(9)); tree.insert(new Node(3)); tree.insert(new Node(7)); tree.insert(new Node(8)); tree.insert(new Node(6)); assertEquals(20, new Algorithm().sumOfVerticalLine(tree, 3)); } @Test public void canEvaluateVerticalSumOfBinaryTree2() { BinaryTree tree = new BinaryTree(new Node(1)); tree.insert(new Node(2)); tree.insert(new Node(3)); tree.insert(new Node(4)); tree.insert(new Node(5)); tree.insert(new Node(6)); tree.insert(new Node(7)); tree.insert(new Node(8)); tree.insert(new Node(9)); tree.insert(new Node(10)); assertEquals(21, new Algorithm().sumOfVerticalLine(tree, 3)); }} document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/01/find-the-vertical-sum-of-binary-tree/"},{"title":"Find the maximum width of binary tree","text":"Given a Binary tree, how will you find the maximum width of Binary Tree? For example, for this Binary tree it has maximum width is 4. Solution We can use level order traversal along with queue to find maximum width. Create an empty queue and start from root node. Loop while queue is not empty. Update maxWidth if count of nodes in queue is greater. Remove all current level nodes from queue. Add all next level nodes into the queue. Algorithm - To find the maximum width of binary tree1234567891011121314151617181920212223242526272829303132333435363738class Algorithm { /** * We can use level order traversal along with queue to find maximum width. * * @param tree binary tree * @return maximum width of binary tree */ public int maxWidth(BinaryTree tree) { Queue&lt;Node&gt; nodes = new LinkedList&lt;&gt;(); int maxWidth = 0; nodes.add(tree.getRoot()); while (!nodes.isEmpty()) { int count = nodes.size(); maxWidth = Math.max(maxWidth, count); while (count-- &gt; 0) { Node node = nodes.remove(); if (node.getLeft() != null) { nodes.add(node.getLeft()); } if (node.getRight() != null) { nodes.add(node.getRight()); } } } return maxWidth; }} Test Scenario1234567891011121314151617181920212223242526272829303132333435363738394041424344public class AlgorithmTest { @Test public void canEvaluateMaxWidthOfBinaryTree1() { BinaryTree tree = new BinaryTree(new Node(5)); tree.insert(new Node(4)); tree.insert(new Node(9)); tree.insert(new Node(3)); tree.insert(new Node(7)); Node node = new Node(8); node.setLeft(new Node(5)); tree.insert(node); tree.insert(new Node(6)); tree.insert(new Node(5)); assertEquals(4, new Algorithm().maxWidth(tree)); } @Test public void canEvaluateMaxWidthOfBinaryTree2() { BinaryTree tree = new BinaryTree(new Node(1)); tree.insert(new Node(2)); tree.insert(new Node(3)); tree.insert(new Node(4)); tree.insert(new Node(5)); tree.insert(new Node(6)); tree.insert(new Node(7)); tree.insert(new Node(8)); tree.insert(new Node(9)); tree.insert(new Node(10)); tree.insert(new Node(11)); tree.insert(new Node(12)); tree.insert(new Node(13)); tree.insert(new Node(14)); tree.insert(new Node(15)); assertEquals(8, new Algorithm().maxWidth(tree)); }} document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/01/find-the-maximum-width-of-binary-tree/"},{"title":"Secure application secrets by using spring vault","text":"Security and working with secrets is a concern of every developer working with databases, user credentials or API keys. Vault steps in by providing a secure storage combined with access control, revocation, key rolling and auditing. In short: Vault is a service for securely accessing and storing secrets. In this post, I’ll walk you through to store secrets into vault for your application. You’ll learn how to setup Spring Cloud Vault to store and read database credentials for your stand-alone application from vault. Start PostgreSQL and Vault locallyYou can download docker-compose.yml file and just run the following to get PostgreSQL and Vault started locally in docker containers. 1docker-compose up -d Creating the VAULT PostgreSQL credentialsLet’s access vault container in your terminal. 123docker exec -it example-vault /bin/shexport VAULT_TOKEN=\"00000000-0000-0000-0000-000000000000\"export VAULT_ADDR=\"http://127.0.0.1:8200\" Once you have access to vault and postgres access, do the following to enable postgres and create a role that the application will use to connect to PostgreSQL. Enable secrets for postgresql 1vault secrets enable postgresql Write postgresql connection url 12vault write postgresql/config/connection \\ connection_url=\"postgresql://user:password@example-vault-db:5432/postgres?sslmode=disable\" Apply lease setting: vault create new username and password for database 1vault write postgresql/config/lease lease=1h lease_max=24h Add role for user in database123vault write postgresql/roles/readonly \\ sql=\"CREATE ROLE \\\"{{name}}\\\" WITH LOGIN PASSWORD '{{password}}' VALID UNTIL '{{expiration}}'; GRANT ALL ON ALL TABLES IN SCHEMA public TO \\\"{{name}}\\\";\" To generate new set of credentials run (Vault is now configured to create and manage credentials for Postgres!) 1vault read postgresql/creds/readonly Output 1234567Key Value--- -----lease_id postgresql/creds/readonly/ihWk1Q9cC3uHgjFBYOBJZFTvlease_duration 1hlease_renewable truepassword 4878ef62-ed3d-098e-22c7-71af6891eaa8username token-954d49c2-9269-451d-9ce9-97e5aa887222 For more info visit vault documentation here: PostgreSQL Secret Engines Create a Spring Boot Project1234567curl https://start.spring.io/starter.tgz -d dependencies=postgresql,cloud-starter-vault-config,data-jpa \\ -d groupId=io.github.bhuwanupadhyay \\ -d artifactId=example-vault \\ -d packageName=io.github.bhuwanupadhyay.tutorial \\ -d baseDir=example-vault \\ -d bootVersion=2.2.2.RELEASE | tar -xzvf -cd example-vault Using VAULT in Spring Boot Project with Spring CloudNeeded maven dependencies under you maven project pom.xml 1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-vault-config&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-vault-config-databases&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.postgresql&lt;/groupId&gt; &lt;artifactId&gt;postgresql&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; The spring cloud use bootstrap.yml or bootstrap.properties file. So, to connect with vault you need to define following values in bootstrap.yml file. 12345678910111213141516spring: application: name: example-vault cloud: vault: uri: http://localhost:8200 token: '00000000-0000-0000-0000-000000000000' database: enabled: true role: readonly backend: postgresql datasource: url: jdbc:postgresql://localhost:5432/postgres jpa: hibernate: ddl-auto: create-drop Database Access: PostgreSQL (read/write)Let’s implement customer entity and its spring data repository as follows: Entity1234567891011121314151617181920212223242526272829303132333435363738394041424344@Entitypublic class Customer { @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id; private String name; public Customer() { } public Customer(String name) { this.name = name; } public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Customer customer = (Customer) o; return Objects.equals(id, customer.id); } @Override public int hashCode() { return Objects.hash(id); }} Entity Data Repository12public interface CustomerRepository extends JpaRepository&lt;Customer, Long&gt; {} Test Scenario12345678910111213141516171819@Spring BootTestclass DemoApplicationTests { @Autowired private CustomerRepository repository; @BeforeEach void setUp() { repository.deleteAll(); } @Test void canAddCustomer() { String name = \"Vault - \" + UUID.randomUUID().toString(); repository.save(new Customer(name)); assertEquals(name, repository.findAll().get(0).getName()); }} You can find example on github: Source Code document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/01/secure-application-secrets-by-using-spring-vault/"},{"title":"How to use jmh with spring boot","text":"Application performance is a concern of every developer working with software. JMH provides an easy API for developer to benchmark application performance. In this post, I’ll walk you through to how to use jmh with spring boot. You’ll learn how to set up Spring Boot Test and how to enable jmh benchmark in your test code. Create a Spring Boot Project1234567curl https://start.spring.io/starter.tgz -d dependencies=h2,data-jpa,data-rest,lombok \\ -d groupId=io.github.bhuwanupadhyay \\ -d artifactId=example \\ -d packageName=io.github.bhuwanupadhyay.tutorial \\ -d baseDir=example \\ -d bootVersion=2.2.2.RELEASE | tar -xzvf -cd example Add JMH dependencies in pom.xml123456789101112&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jmh&lt;/groupId&gt; &lt;artifactId&gt;jmh-core&lt;/artifactId&gt; &lt;version&gt;1.22&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jmh&lt;/groupId&gt; &lt;artifactId&gt;jmh-generator-annprocess&lt;/artifactId&gt; &lt;version&gt;1.22&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; Let’s add some classes for benchmark OrderLine entity class 123456789101112131415@Entity@Getter@Setter@ToString(exclude = {\"itemId\", \"addressLine\", \"quantity\"})@EqualsAndHashCode(exclude = {\"itemId\", \"addressLine\", \"quantity\"})class OrderLine implements Serializable { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private Long itemId; private String addressLine; private Integer quantity;} OrderLineRepository repository class for an entity OrderLine 123interface OrderLineRepository extends JpaRepository&lt;OrderLine, Long&gt; {} The spring boot configuration application.properties 12spring.application.name=examplespring.jpa.hibernate.ddl-auto=create-drop JMH Benchmark CodeLet’s benchmark insert operation for OrderLine entity into database. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Spring BootTest@State(Scope.Benchmark)@BenchmarkMode(Mode.AverageTime)@OutputTimeUnit(TimeUnit.MILLISECONDS)public class DemoApplicationTests { private static OrderLineRepository repository; @Autowired public void setRepository(OrderLineRepository repository) { DemoApplicationTests.repository = repository; } @Test public void runBenchmarks() throws Exception { Options opts = new OptionsBuilder() // set the class name regex for benchmarks to search for to the current class .include(\"\\\\.\" + this.getClass().getSimpleName() + \"\\\\.\") .warmupIterations(3) .measurementIterations(3) // do not use forking or the benchmark methods will not see references stored within its class .forks(0) // do not use multiple threads .threads(1) .shouldDoGC(true) .shouldFailOnError(true) .jvmArgs(\"-server\") .build(); new Runner(opts).run(); } @Benchmark public void dbInserts(Parameters parameters) { int size = Integer.parseInt(parameters.batchSize); for (int i = 0; i &lt; size; i++) { OrderLine line = new OrderLine(); line.setAddressLine(\"Jhamsikhel Ward #3, Arun Thapa Chwok, Lalitpur, Nepal\"); line.setItemId(1L); line.setQuantity(i); repository.save(line); } } @State(value = Scope.Benchmark) public static class Parameters { @Param({\"1\", \"1000\"}) String batchSize; }} Benchmark Result for this example document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/01/how-to-use-jmh-with-spring-boot/"},{"title":"Determine Time Complexity of Algorithms","text":"Big-O notation, sometimes called asymptotic notation, is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. In computer science, Big-O notation is used to classify algorithms according to how their running time or space requirements grow as the input size (n) grows. This notation characterizes functions according to their growth rates: different functions with the same growth rate may be represented using the same O notation. Table of common time complexities123456789| Name | Complexity | Description | |-------------------|---------------|-----------------------------------------------------------------------------------------------|| Constant Time | O(1) | Not dependent on the input data (n), the running time will always be the same. || Logarithmic Time | O(log n) | When it reduces the size of the input data in each step (it don’t need to look at all values of the input data). || Linear Time | O(n) | When the running time increases at most linearly with the size of the input data. || Quasilinear Time | O(n log n) | When each operation in the input data have a logarithm time complexity. || Quadratic Time | O(n^2) | When it needs to perform a linear time operation for each value in the input data. || Exponential Time | O(2^n) | When the growth doubles with each addition to the input data set. || Factorial Time | O(n!) | When it grows in a factorial way based on the size of the input data. | Examples of common time complexities123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104public class ExamplesOfCommonTimeComplexities { /* Constant Time - O(1) */ public void constantTime(int a, int b) { if (a &gt; b) System.out.println(\"A is greater than b.\"); else System.out.println(\"B is greater than a.\"); } /* Logarithmic Time - O(log n) */ public void logarithmicTime(int[] a) { int k = a.length / 2; for (int i = 0; i &lt; k; i++) { System.out.print(a[i] + \" \"); } } /* Linear Time - O(n) */ public void linearTime(int[] a) { for (int i = 0; i &lt; a.length; i++) { System.out.print(a[i] + \" \"); } } /* Quasilinear Time - O(n log n) */ public void quasilinearTime(int[] a) { for (int i = 0; i &lt; a.length; i++) { logarithmicTime(a); System.out.print(a[i] + \" \"); } } /* Quadratic Time - O(n^2) */ public void quadraticTime(int[] a, int[] b) { for (int i = 0; i &lt; a.length; i++) { for (int j = 0; j &lt; b.length; j++) { System.out.print(a[i] + \",\" + b[j] + \" \"); } } } /* Exponential Time - O(2^n) Example: def fibonacci(n): if n &lt;= 1: return n return fibonacci(n-1) + fibonacci(n-2) */ /* Factorial Time - O(n!) Example: def heap_permutation(data, n): if n == 1: print(data) return for i in range(n): heap_permutation(data, n - 1) if n % 2 == 0: data[i], data[n-1] = data[n-1], data[i] else: data[0], data[n-1] = data[n-1], data[0] data = [1, 2, 3] heap_permutation(data, len(data)) */} Analyzing the time complexity of an algorithmWhen analyzing the time complexity of an algorithm with several operations we need to describe the algorithm based on the largest complexity among all operations. For Example: The algorithm to revers the orders of words 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/* Question - Given a string with multiple words and spaces represented as a character array. Write an in-place algorithm to reverse the order of words in the string. Example: CONVERT ['p', 'e', 'r', 'f', 'e', 'c', 't', ' ', 'm', 'a', 'k', 'e', 's', ' ', 'p', 'r', 'a', 'c', 't', 'i', 'c', 'e'] TO ['p', 'r', 'a', 'c', 't', 'i', 'c', 'e', ' ', 'm', 'a', 'k', 'e', 's', ' ', 'p', 'e', 'r', 'f', 'e', 'c', 't'] */public class ReverseWordInString { /** * @param a - an array * @return - reverse character array by space * &lt;p&gt; * Time Complexity - O(n log n) */ public static char[] reverse(char[] a) { Stack&lt;Character&gt; stack = new Stack&lt;&gt;(); char[] result = new char[a.length]; int startAt = 0; for (int i = a.length - 1; i &gt; -1; i--) { if (a[i] != ' ') { stack.push(a[i]); } boolean isComma = a[i] == ' '; if (isComma || i == 0) { while (!stack.isEmpty()) { result[startAt++] = stack.pop(); } if (isComma) { result[startAt++] = a[i]; } } } return result; }} Let’s analyze each operations: The for loop run from last index to first index i.e. O(n). Inside the for loop we have while loop that run till next ' ' from array of characters i.e. O(log n). So, total time complexity of this for loop is O(n log n). Other operations have constant time complexity. Now, for this algorithms we have O(n log n) is the largest complexity among all operations. Therefore, we can describe this algorithm has time complexity as O(n log n). Big-O Cheat SheetThe graph that shows running time complexity in terms of big-o notation. Here you can find a sheet with the time complexity of the operations in the most common data structures. Here is another sheet with the time complexity of the most common sorting algorithms. References Big-O notation: https://en.wikipedia.org/wiki/Big_O_notation Big-O Cheat Sheet: https://www.bigocheatsheet.com/ document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/02/determine-time-complexity-of-algorithms/"},{"title":"Highly scalable database designs","text":"While architecting cloud-native applications, you need to ensure that your system is highly available, performant, scalable, fault-tolerant, and has the capability to recover from a disaster scenario. In this article, Samir Behara discusses the options available when designing the database architecture to achieve scalability. For further detail, reading see an article on Reg Gate In this post, I will explain how we can deploy highly scalable PostgresSQL databases using Docker. Also, I will talk about how to connect our microservices (based on Spring Boot) with scaled database deployments via api gateway by using practical examples. Example Scenario Query APIs - on Github12345@Beanpublic RouterFunction&lt;ServerResponse&gt; employeeRoutes(EmployeeHandler handler) { return route(GET(\"/employees/{id}\").and(accept(APPLICATION_JSON)), handler::getEmployee) .andRoute(GET(\"/employees\").and(accept(APPLICATION_JSON)), handler::listEmployee);} Command APIs - on Github123456@Beanpublic RouterFunction&lt;ServerResponse&gt; employeeRoutes(EmployeeHandler handler) { return route(DELETE(\"/employees/{id}\").and(accept(APPLICATION_JSON)), handler::deleteEmployee) .andRoute(PUT(\"/employees/{id}\").and(accept(APPLICATION_JSON)), handler::updateEmployee) .andRoute(POST(\"/employees\").and(accept(APPLICATION_JSON)), handler::createEmployee);} API Gateway with KrakenD - on GithubKrakenD is a stateless, distributed, high-performance API Gateway that helps you effortlessly adopt microservices. The configuration file for KrakenD to address our requirements: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172{ \"version\": 2, \"endpoints\": [ { \"endpoint\": \"/v1/employees\", \"method\": \"GET\", \"backend\": [ { \"url_pattern\": \"/employees\", \"method\": \"GET\", \"host\": [ \"http://query:8080\" ] } ] }, { \"endpoint\": \"/v1/employees/{id}\", \"method\": \"GET\", \"backend\": [ { \"url_pattern\": \"/employees/{id}\", \"method\": \"GET\", \"host\": [ \"http://query:8080\" ] } ] }, { \"endpoint\": \"/v1/employees\", \"method\": \"POST\", \"backend\": [ { \"url_pattern\": \"/employees\", \"method\": \"POST\", \"host\": [ \"http://command:8080\" ] } ] }, { \"endpoint\": \"/v1/employees/{id}\", \"method\": \"DELETE\", \"backend\": [ { \"url_pattern\": \"/employees/{id}\", \"method\": \"DELETE\", \"host\": [ \"http://command:8080\" ] } ] }, { \"endpoint\": \"/v1/employees/{id}\", \"method\": \"PUT\", \"backend\": [ { \"url_pattern\": \"/employees/{id}\", \"method\": \"PUT\", \"host\": [ \"http://command:8080\" ] } ] } ], \"extra_config\": { }} Docker Stack - on GithubYou can define number of replicas as you need for high scalable database design. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354...services: # ------------------------------------------------- # Application Backend Databases # ------------------------------------------------- replica: image: bitnami/postgresql:12 ... deploy: replicas: 3 ... primary: image: bitnami/postgresql:12 ... deploy: replicas: 1 ... # ------------------------------------------------- # Api Gateway # ------------------------------------------------- gateway: image: devopsfaith/krakend:1.1.1-alpine ... deploy: replicas: 1 ... # ------------------------------------------------- # Applications # ------------------------------------------------- command: image: bhuwanupadhyay/employees-command ... deploy: replicas: 1 ... query: image: bhuwanupadhyay/employees-query ... deploy: replicas: 1 ... ui: image: bhuwanupadhyay/employees-ui ... deploy: replicas: 1 ... Deploy in Docker SwarmLet’s build the docker images that needed on this scenario. Firstly, clone the github repo and checkout the branch 13-highly-scalable-database-designs then run following commands: 1make build &amp;&amp; docker-compose build --no-cache To enable a docker swarm in your machine: 1docker swarm init To deploy using docker-compose.stack.yml 1docker stack deploy --compose-file docker-compose.stack.yml highly-scalable-db Testing APIsFor testing, we need a host and port for api gateway. The default port for api gateway is 8080, so to identify a host ip address lets run docker network inspect command for gateway. 1docker network inspect highly-scalable-db_gateway | grep 'IP\\\"' 12export API_HOST=&lt;IP Address&gt;export API_HOST_PORT=8080 Install httpie tool on your machine. Install Documentation Create New Employee1http POST $API_HOST:$API_HOST_PORT/v1/employees name=\"Bhuwan Prasad Upadhyay\" List Employees1http $API_HOST:$API_HOST_PORT/v1/employees Get Employee by Id1http $API_HOST:$API_HOST_PORT/v1/employees/&lt;employeeId&gt; Update an Employee1http PUT $API_HOST:$API_HOST_PORT/v1/employees/&lt;employeeId&gt; name=\"Bandana Poudyal\" Delete an Employee1http DELETE $API_HOST:$API_HOST_PORT/v1/employees/&lt;employeeId&gt; document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/03/highly-scalable-database-designs/"},{"title":"Find the longest palindromic contiguous substring","text":"Given a string, find the longest palindromic contiguous substring.If there are more than one with the maximum length, return any one. ExampleFor example, the longest palindromic substring of aabcdcb is bcdcb. The longest palindromic substring of bananas is anana. Solution - by Manacher Algorithm12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class Solution { String longestPalindrome(String s) { char[] t = preprocess(s); int[] p = new int[t.length]; int center = 0, right = 0; for (int i = 1; i &lt; t.length - 1; i++) { int mirror = 2 * center - i; if (right &gt; i) { p[i] = Math.min(right - i, p[mirror]); } // attempt to expand palindrome centered at i while (t[i + (1 + p[i])] == t[i - (1 + p[i])]) { p[i]++; } // if palindrome centered at i expands past right, // adjust center based on expanded palindrome. if (i + p[i] &gt; right) { center = i; right = i + p[i]; } } return longestPalindromicSubstring(p, s); } private String longestPalindromicSubstring(int[] p, String s) { int length = 0; // length of longest palindromic substring int center = 0; // center of longest palindromic substring for (int i = 1; i &lt; p.length - 1; i++) { if (p[i] &gt; length) { length = p[i]; center = i; } } return s.substring((center - 1 - length) / 2, (center - 1 + length) / 2); } // Transform s into t. // For example, if s = \"abba\", then t = \"$#a#b#b#a#@\" // the # are interleaved to avoid even/odd-length palindromes uniformly // $ and @ are prepended and appended to each end to avoid bounds checking private char[] preprocess(String s) { char[] t = new char[s.length() * 2 + 3]; t[0] = '$'; t[s.length() * 2 + 2] = '@'; for (int i = 0; i &lt; s.length(); i++) { t[2 * i + 1] = '#'; t[2 * i + 2] = s.charAt(i); } t[s.length() * 2 + 1] = '#'; return t; }} Test Cases1234567891011121314151617181920212223242526272829class SolutionTest { private Solution solution; static Collection&lt;Object[]&gt; data() { return Arrays.asList( new Object[][]{ {\"aabcdcb\", \"bcdcb\"}, {\"bananas\", \"anana\"}, } ); } @BeforeEach void setUp() { this.solution = new Solution(); } @ParameterizedTest @MethodSource(\"data\") void testSolution(String input, String expected) { String actual = this.solution.longestPalindrome(input); assertEquals(expected, actual); }} Analysis Time Complexity: O(n) Space Complexity: O(n) document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/03/find-the-longest-palindromic-contiguous-substring/"},{"title":"Generate random integers in range using Java","text":"Sometimes, one might need to assign a random value to a variable. Random numbers within a specific range of type integer, float, double, long, boolean can be generated in Java. There are three methods to generate random numbers in Java. 1. By using - Math.randomThis Math.random() gives a random double from 0.0 (inclusive) to 1.0 (exclusive). For generating random numbers within a range using Math.random(), see the example below: 12345678910111213141516public class Randomizer { /** * @param min - minimum range value * @param max - maximum range value * @return random integer between min (inclusive) to max (inclusive) with uniform probability */ public static int next(int min, int max) { if (min &gt;= max) { throw new IllegalArgumentException(\"max must be greater than min\"); } return (int)(Math.random() * ((max - min) + 1)) + min; }} 2. By using - java.util.RandomFor generating random numbers within a range using java.util.Random, see the example below: 1234567891011121314151617public class Randomizer { /** * @param min - minimum range value * @param max - maximum range value * @return random integer between min (inclusive) to max (inclusive) with uniform probability */ public static int next(int min, int max) { if (min &gt;= max) { throw new IllegalArgumentException(\"max must be greater than min\"); } Random r = new Random(); return r.nextInt((max - min) + 1) + min; }} The Random.nextInt(n) is more efficient than Math.random() * n, read this post. In addition, Math.random() is thread safe by itself but if you want to generate numbers using Random class then ThreadLocalRandom is more preferable which thread safe. 3. [Java 8] By using - Random.intsFor generating random numbers within a range using Random.ints, see the example below: 1234567891011121314151617public class Randomizer { /** * @param min - minimum range value * @param max - maximum range value * @return random integer between min (inclusive) to max (inclusive) with uniform probability */ public static int next(int min, int max) { if (min &gt;= max) { throw new IllegalArgumentException(\"max must be greater than min\"); } Random r = new Random(); return r.ints(min, (max + 1)).limit(1).findFirst().getAsInt(); }} Note: To generates random integers in a range between 15 (inclusive) and 20 (exclusive), with stream size of 3. 1new Random().ints(3, 15, 20).forEach(System.out::println); document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/03/generate-random-integers-in-range-using-java/"},{"title":"Implement rand7 using rand5","text":"Using a function rand5() that returns an integer from 1 to 5 (inclusive) with uniform probability, implement a function rand7() that returns an integer from 1 to 7 (inclusive). Do NOT use system’s Math.random() Example12345Input: 2Output: [7,4]Input: 3Output: [3,1,7] Solution - by Rejection SamplingThis solution is based upon Rejection Sampling. The main idea is when you generate a number in the desired range, output that number immediately. If the number is out of the desired range, reject it and re-sample again. As each number in the desired range has the same probability of being chosen, a uniform distribution is produced. Obviously, we have to run rand5() function at least twice, as there are not enough numbers in the range of 1 to 7. By running rand5() twice, we can get integers from 1 to 25 uniformly. Since 25 is not a multiple of 7, we have to use rejection sampling. Our desired range is integers from 1 to 21, which we can return the answer immediately. If not (the integer falls between 22 to 25), we reject it and repeat the whole process again. 1234567891011121314151617181920212223class Solution { /** * @return random integer 1 to 5 (inclusive) with uniform (or equal) probability */ private int rand5() { int min = 1; int max = 5; Random r = new Random(); return r.nextInt((max - min) + 1) + min; } int rand7() { int row, col, idx; do { row = rand5(); col = rand5(); idx = col + (row - 1) * 5; } while (idx &gt; 21); return 1 + (idx - 1) % 7; }} Test Cases123456789101112131415161718192021class SolutionTest { private Solution solution; @BeforeEach void setUp() { this.solution = new Solution(); } @Test void testSolution() { IntStream.range(0, 1000) .mapToObj(value -&gt; this.solution.rand7()) .forEach(random -&gt; { assertTrue( random &gt; 0 &amp;&amp; random &lt;= 7, String.format(\"random number: %d -&gt; is not in range {1, 7} inclusive\", random) ); }); }} Analysis Time Complexity: O(1) average, but O(∞) worst case. Space Complexity: O(1) document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/03/implement-rand7-using-rand5/"},{"title":"Merge sort","text":"Merge sort is a divide-and-conquer algorithm based on the idea of breaking down a list into several sub-lists until each sublist consists of a single element and merging those sublists in a manner that results into a sorted list. Main Idea Divide the unsorted list into N sublists, each containing element. Take adjacent pairs of two singleton lists and merge them to form a list of 2 elements. N will now convert into N/2 lists of size 2. Repeat the process till a single sorted list of obtained. Example Implementation in Java12345678910111213141516171819202122232425262728293031323334353637383940414243444546class MergeSort { private void merge(int[] A, int start, int mid, int end) { //stores the starting position of both parts in temporary variables. int p = start, q = mid + 1; int[] Arr = new int[end - start + 1]; int k = 0; for (int i = start; i &lt;= end; i++) { if (p &gt; mid) //checks if first part comes to an end or not . Arr[k++] = A[q++]; else if (q &gt; end) //checks if second part comes to an end or not Arr[k++] = A[p++]; else if (A[p] &lt; A[q]) //checks which part has smaller element. Arr[k++] = A[p++]; else Arr[k++] = A[q++]; } for (int i = 0; i &lt; k; i++) { /* Now the real array has elements in sorted manner including both parts. */ A[start++] = Arr[i]; } } private void mergeSort(int[] A, int start, int end) { if (start &lt; end) { int mid = (start + end) / 2; // defines the current array in 2 parts . mergeSort(A, start, mid); // sort the 1st part of array . mergeSort(A, mid + 1, end); // sort the 2nd part of array. // merge the both parts by comparing elements of both the parts. merge(A, start, mid, end); } } void mergeSort(int[] A) { this.mergeSort(A, 0, A.length - 1); }} Test Cases123456789101112131415161718192021222324252627class MergeSortTest { private MergeSort solution; static Collection&lt;Object[]&gt; data() { return Arrays.asList( new Object[][]{ {new int[]{5, 7, 8, 9, 2}, new int[]{2, 5, 7, 8, 9}}, {new int[]{9, 7, 8, 3, 2, 1}, new int[]{1, 2, 3, 7, 8, 9}}, } ); } @BeforeEach void setUp() { this.solution = new MergeSort(); } @ParameterizedTest @MethodSource(\"data\") void testSolution(int[] input, int[] expected) { this.solution.mergeSort(input); assertArrayEquals(expected, input); }} Analysis Time Complexity: O(n log(n)) Space Complexity: O(n) Merge Sort is a stable sort which means that the same element in an array maintain their original positions with respect to each other. Overall time complexity of Merge sort is O(nLogn). It is more efficient as it is in the worst case also the runtime is O(nlogn). document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/03/merge-sort/"},{"title":"Implement a URL shortener with alphanumeric string","text":"Implement a URL shortener with six-character alphanumeric string. ExampleImplement a URL shortener with the following methods: shorten(url), which shortens the url into a six-character alphanumeric string, such as zLg6wl. restore(short), which expands the shortened string into the original url. If no such shortened string exists, return null. What if we enter the same URL twice? Solution12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution { public static final int MAX = 6; private final Map&lt;URL, URL&gt; urlCache = new HashMap&lt;&gt;(); private final AlphanumericRandomizer randomizer = new AlphanumericRandomizer(); URL shorten(String url) { try { URL longUrl = URI.create(url).toURL(); String shortValue = randomizer.next(MAX); URL shortUrl = new URL(longUrl.getProtocol(), longUrl.getHost(), \"/\" + shortValue); this.urlCache.put(shortUrl, longUrl); return shortUrl; } catch (MalformedURLException e) { throw new RuntimeException(e); } } URL restore(URL shortValue) { return this.urlCache.get(shortValue); } /** * Alphanumeric characters are A to Z, a to z and 0 to 9 */ static class AlphanumericRandomizer { private static final String CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"; private final Random r; private final String alphaNumeric; public AlphanumericRandomizer() { r = new Random(); this.alphaNumeric = CHARS + CHARS.toLowerCase() + \"0123456789\"; } public String next(int size) { final int length = this.alphaNumeric.length(); return IntStream.range(0, size) .mapToObj(value -&gt; r.nextInt(length)) .map(index -&gt; String.valueOf(alphaNumeric.charAt(index))) .collect(Collectors.joining()); } }} Test Cases1234567891011121314151617181920212223242526272829303132333435363738class SolutionTest { private Solution solution; static Collection&lt;Object[]&gt; data() { return Arrays.asList(new Object[][]{ {\"http://hello.com/test\", \"http\", \"hello.com\", \"/[A-Za-z0-9]{6}\"}, {\"http://hello.com/test\", \"http\", \"hello.com\", \"/[A-Za-z0-9]{6}\"}, {\"https://hello.com/test\", \"https\", \"hello.com\", \"/[A-Za-z0-9]{6}\"}, }); } @BeforeEach void setUp() { this.solution = new Solution(); } @ParameterizedTest @MethodSource(\"data\") void testSolution(String url, String expectedProtocol, String expectedHost, String expectedPathRegEx) { URL actual = this.solution.shorten(url); assertEquals(expectedProtocol, actual.getProtocol()); assertEquals(expectedHost, actual.getHost()); assertTrue(actual.getPath().matches(expectedPathRegEx), \"The shorten url \" + actual + \" is not six characters alphanumeric.\"); } @ParameterizedTest @MethodSource(\"data\") void testSolution(String url) throws MalformedURLException { URL shorten = this.solution.shorten(url); URL actual = this.solution.restore(shorten); URL expected = URI.create(url).toURL(); assertEquals(expected, actual); }} document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/03/implement-a-url-shortener-with-alphanumeric-string/"},{"title":"Applying semantic versioning with git repository","text":"Semantic versioning is the practice of assigning version numbers based on the severity of the change. For semantic versioning, the Semver used as a versioning standard, and it based on the Conventional Commits. By using the node library: semantic-release we can automate the whole package release workflow including: determining the next version number, generating the release notes and publishing the package. In this post, I will explain how we can integrate semantic versioning in the git repository. Conventional CommitsTo be used, semantic release needs commit message to follow a precise pattern. Here is the expected syntax for commit messages. 12345&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;[optional body][optional footer(s)] The commit contains the following structural elements, to communicate intent to the consumers of your library: fix ➜ patches a bug in your codebase [PATCH] feat ➜ introduces a new feature to the codebase [MINOR] BREAKING CHANGE ➜ (a commit that has a footer BREAKING CHANGE:, or appends a ! after the type/scope) introduces a breaking API change [MAJOR] build, chore, ci, docs, style, refactor, revert, perf, test ➜ other types than fix and feat footers other than BREAKING CHANGE: &lt;description&gt; may be provided and follow a convention similar to git trailer format &lt;subject&gt; - succinct description of the change, use the imperative, present tense: change not changed nor changes, don’t capitalize first letter, no dot (.) at the end &lt;scope&gt; - We can insert as scope references to our JIRA/GITHUB/GITLAB issues id(XXX-1223) Enforce Conventional CommitsThe automated versioning only work fully when all the team respect these rules. To ensure automated versioning, we need to add some custom hooks in your versioning system. Remote hooks setup on Github or Gitlab Github: Configure pre-receive hooks for an organization Gitlab: Set a global server hook for all repositories Local hooks setup for the conventional commit: Run following commands in git repository directory 12echo \"module.exports = {extends: ['@commitlint/config-conventional']};\" &gt; commitlint.config.jsecho '{\"hooks\":{\"commit-msg\":\"commitlint -E HUSKY_GIT_PARAMS\"}}' &gt; .huskyrc Install commitlint and husky : 12npm initnpm install --save-dev husky @commitlint/{cli,config-conventional} Next git commit if you don’t follow conventional commit patterns then it will reject the commit. Semantic Release Install node library: semantic-release 1npm i -g semantic-release @semantic-release/{git,exec,changelog} Create .releaserc in your repository 1234567891011121314{ \"branches\": [\"master\"], \"plugins\": [ \"@semantic-release/commit-analyzer\", \"@semantic-release/release-notes-generator\", \"@semantic-release/github\", \"@semantic-release/git\", \"@semantic-release/changelog\", [\"@semantic-release/exec\", { \"prepareCmd\" : \"echo '${nextRelease.version} run your custom shell script'\", \"publishCmd\" : \"echo 'Published.......run your custom shell script'\" }] ]} To run locally in dry run mode from repository 12export GITHUB_TOKEN=&lt;token&gt; from https://github.com/settings/tokens (repo Permission)semantic-release --dry-run If you want to integrate semantic release with your CICD pipelines visit this documentation. I have enabled semantic release with Github actions for my maven project factory-parent. CI Action Workflows Release Artifacts The sample yaml file for Github action workflow to run semantic-release as below: 123456789101112131415161718192021name: Releaseon: [ push ]jobs: release: needs: build name: Semantic Release runs-on: ubuntu-latest if: github.ref == 'refs/heads/master' steps: # check out repository code and setup node - uses: actions/checkout@v2 - uses: actions/setup-node@v1 with: node-version: \"12.x\" # install dependencies and run semantic-release - run: npm i -g semantic-release @semantic-release/{git,exec,changelog} - run: semantic-release env: GITHUB_TOKEN: &lt;github_token&gt; document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/04/applying-semantic-versioning-with-git-repository/"},{"title":"Domain-Driven Design Building Blocks","text":"Domain-driven design (DDD), is an approach used to build systems that have a complex business domain. So you wouldn’t apply DDD to, say, infrastructure software or building routers, proxies, or caching layers, but instead to business software that solves real-world business problems. It’s a great technique for separating the way the business is modeled from the plumbing code that ties it all together. Separating these two in the software itself makes it easier to design, model, build and evolve an implementation over time. In tactical DDD, the building blocks play an important role in how business is modeled into the code. In this article, I will take you through the best available options for building blocks in object-oriented principles. Value Object An object that represents a descriptive aspect of the domain with no conceptual identity is called a Value Object. Value Objects are instantiated to represent elements of the design that we care about only for what they are, not who or which they are. — Eric Evans In other words, value objects don’t have their own identity. The value object possess concept of structural equality — if two objects are equal then they have equivalent content. Also, If two value objects have the same set of attributes we can treat them interchangeably. Attributes No Identity - value objects are identity-less. Immutable - value object can be replaced by another value object with same content. To make sure, equality by structure for value object is by using Immutable design especially in multi-thread scenarios (immutable objects are threadsafe by design). Lifespan - can’t exist without a parent entity (should not have separate table in a database). Business Constraints - value object is always valid (should need to validate business rules on creation). I always prefer to validate business constraints on the constructor for the value object. Code ExampleEquality logic implementation for value object is very important to ensure they are equals by its content. It’s too easy to forget to override equals() and hashCode() in the value object. It’s very important to make sure all properties of value object should be part of equality logic. To enforce equality logic for value object I used following class as a base class for every value object. 1234567public abstract class ValueObject { @Override public abstract int hashCode(); @Override public abstract boolean equals(Object o);} Entity Many objects are not fundamentally defined by their attributes, but rather by a thread of continuity and identity. — Eric Evans The entity possess concept of identifier equality — Two instances of entity would be equal if they have the same identifiers. We can change everything related to an entity (except its identifier), and after modification also it remains the same entity. Code ExampleIdentifier equality means that entity class has a field for an identifier. Following class can be used as a base class for entity class. 1234567891011121314151617181920212223242526272829import java.util.Objects;public abstract class Entity&lt;ID extends ValueObject&gt; { public static final String ENTITY_ID_IS_REQUIRED = \"EntityIdIsRequired\"; private final ID id; public Entity(ID id) { DomainAsserts.begin(id).notNull(DomainError.create(this, ENTITY_ID_IS_REQUIRED)).end(); this.id = id; } public ID getId() { return this.id; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || this.getClass() != o.getClass()) return false; Entity&lt;?&gt; entity = (Entity&lt;?&gt;) o; return Objects.equals(this.id, entity.id); } @Override public int hashCode() { return Objects.hash(this.id); }} Domain Event The essence of a Domain Event is that you use it to capture things that can trigger a change to the state of the application you are developing. — Martin Fowler A domain event is, something that happened in the domain that you want notify to other parts of the same domain. The important benefit of domain events is that side effects can be expressed explicitly. Code ExampleAccording to nature side effects, the published domain event can be listened inside same bounded context or another bounded context. Following class can be used as a base class for domain event. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import java.util.Objects;import java.util.UUID;public abstract class DomainEvent { private final String eventId = UUID.randomUUID().toString(); private final String eventClassName = getClass().getName(); private final String domainEventType; public DomainEvent(DomainEventType domainEventType) { DomainAsserts.begin(domainEventType) .notNull(DomainError.create(this, \"DomainEventTypeIsRequired\")) .end(); this.domainEventType = domainEventType.name(); } public String getEventId() { return eventId; } public String getEventClassName() { return eventClassName; } private String getDomainEventType() { return domainEventType; } public boolean isInsideContext() { return Objects.equals(DomainEventType.BOTH.name(), this.getDomainEventType()) || Objects.equals(DomainEventType.INSIDE.name(), this.getDomainEventType()); } public boolean isOutsideContext() { return Objects.equals(DomainEventType.BOTH.name(), this.getDomainEventType()) || Objects.equals(DomainEventType.OUTSIDE.name(), this.getDomainEventType()); } public enum DomainEventType { /** Represents domain event is inside same bounded context only. */ INSIDE, /** Represents domain event is only for other bounded context. */ OUTSIDE, /** * Represents domain event is available for both i.e. inside same bounded context and other * bounded context. */ BOTH }} Aggregate Root An AGGREGATE is a cluster of associated objects that we treat as a unit for the purpose of data changes. Each aggregate has a root and a boundary. — Eric Evans Group the entities and value objects into aggregates and define boundaries around each. Control all access to the objects inside the boundary through the root. Allow external objects to hold references to the root only. Register domain events into the aggregate root. Code ExampleThe example of base class for aggregate root in my tactical ddd implementation as below. 12345678910111213141516171819202122232425262728293031import java.util.ArrayList;import java.util.Collections;import java.util.List;public abstract class AggregateRoot&lt;ID extends ValueObject&gt; extends Entity&lt;ID&gt; { private final transient List&lt;DomainEvent&gt; domainEvents = new ArrayList&lt;&gt;(); public AggregateRoot(ID id) { super(id); } /** * Register domain event * * @param event domain event */ protected void registerEvent(DomainEvent event) { this.domainEvents.add(event); } /** Clear registered domain events. */ public void clearDomainEvents() { this.domainEvents.clear(); } /** @return list of domain events */ public List&lt;DomainEvent&gt; getDomainEvents() { return Collections.unmodifiableList(domainEvents); }} RepositoriesA repository is a service that uses a global interface to provide access to all entities and value objects that are within a particular aggregate collection. Code ExampleFrom a repository, we have to publish all domain events when persist the aggregate root and then detached domain events from the aggregate root. You can use following code to extend your domain repository in your project. 123456789101112131415161718192021import java.util.Optional;public abstract class DomainRepository&lt;T extends AggregateRoot&lt;ID&gt;, ID extends ValueObject&gt; { private final DomainEventPublisher publisher; protected DomainRepository(DomainEventPublisher publisher) { this.publisher = publisher; } public abstract Optional&lt;T&gt; findOne(ID id); public void save(T entity) { DomainAsserts.begin(entity).notNull(DomainError.create(this, \"EntityIsRequired\")).end(); this.persist(entity); entity.getDomainEvents().forEach(publisher::publish); entity.clearDomainEvents(); } protected abstract void persist(T entity);} Domain Service Some concepts from the domain aren’t natural to model as objects…a service tends to be named for an activity, rather than an entity — a verb rather than a noun. — Eric Evans A service that expresses a business logic that is not part of any Aggregate Root. When an operation does not conceptually belong to any object. Following the natural contours of the problem, you can implement these operations in services. — Wikipedia Some business rules don’t make sense to be part of an Aggregate. If something is ‘outside’ an Aggregate, then it’s probably is a Domain Service. FactoriesFactories are often used to create Aggregates. Aggregates provide encapsulation and a consistent boundary around a group of objects. Aggregates are important because they enforce the internal consistency of the object they are responsible for. A Factory can be useful when creating a new Aggregate because it will encapsulate the knowledge required to create an Aggregate in a consistent state and with all invariants enforced. You can find example on github: Source Code document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/05/domain-driven-design-building-blocks/"},{"title":"Safely Exchanging Information Across Microservices","text":"Exchanging information between the microservices without breaking existing functionalities is a challenging task specially if your business model evolve with the time. It’s very important to ensure new updates on the microservices should be seamless and have backward compatibility. Specially, if our microservices communicate each other by using pub/sub architecture and have multiple producers and consumers, it is necessary for all those microservices to agree on a contract that is based on a schema. Because, to accommodate new business requirements the message payload structure might needs to evolve, and the existing components are still required to continue to work. In this article, I will take you through how we can used evolving schemas to exchange information between microservices using Spring Boot and Spring Cloud. Avro SchemaAvro is used to define the schema for a message’s payload. This schema describes the fields allowed in the payload, along with their data types. Avro bindings are used to serialize values before writing them, and to deserialize values after reading them. The usage of these bindings requires your applications to use the Avro data format, which means that each payload is associated with a schema. In addition, Avro makes use of the Jackson APIs for parsing JSON. This is likely to be of interest to you if you are familiar with a JSON-based system. Schema RegistryFor evolving schemas, we need to register them somewhere to share between the microservices without any manual updates. This leads to the consumers have to read schema definitions from a registry and publisher needs to provide schema definitions to a registry. To address this philosophy, the concept of schema registry come into the picture. Spring Cloud Schema Registry provides support for schema evolution so that the data can be evolved over time and still work with older or newer producers and consumers and vice versa. Schema Registry ServerTo use Spring Cloud Schema Registry Server in a Maven Spring Boot projects, we need to have spring-cloud-schema-registry-server from Spring Cloud in the project pom.xml: 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-schema-registry-server&lt;/artifactId&gt;&lt;/dependency&gt; To enable schema registry server in spring boot, we need to use the annotation @EnableSchemaRegistryServer on the main application class: 12345678@Spring BootApplication@EnableSchemaRegistryServerpublic class SchemaRegistryApplication { public static void main(String[] args) { SpringApplication.run(SchemaRegistryApplication.class, args); }} Schema registry server uses 8990 as a default port for application and the example of http GET request to fetch schemas by its id will be looks like below: 1curl -X GET http://localhost:8990/schemas/&lt;id&gt; Schema Registry ClientTo use Spring Cloud Schema Registry Client in a Maven Spring Boot projects, we need to have spring-cloud-schema-registry-client from Spring Cloud in the project pom.xml: 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-schema-registry-client&lt;/artifactId&gt;&lt;/dependency&gt; To enable schema registry client in spring boot, we need to use the annotation @EnableSchemaRegistryCLient on the main application class: 12345678@Spring BootApplication@EnableSchemaRegistryClientpublic class OrderServiceApplication { public static void main(String[] args) { SpringApplication.run(OrderServiceApplication.class, args); }} Story – Exchanging InformationLet’s consider we have to exchange messages between Order Service and Payment Service. Order and Payment microservices solution is below: As an above diagram, we have three domain events that are OrderPlaced, PaymentRequested and Payment Received. A sequence diagram for order workflow as below: Create Avro SchemasLet’s create first version avro schemas for PaymentRequested and Payment Received because those domain events are not consumed inside bounded context. PaymentRequestedV1 will publish by Order Service 123456789101112131415{ \"name\": \"PaymentRequestedV1\", \"type\": \"record\", \"namespace\": \"io.github.bhuwanupadhyay.schemas\", \"fields\": [ { \"name\": \"orderId\", \"type\": \"string\" }, { \"name\": \"orderAmount\", \"type\": \"string\" } ]} PaymentReceivedV1 will publish by Payment Service 123456789101112131415{ \"name\": \"PaymentReceivedV1\", \"type\": \"record\", \"namespace\": \"io.github.bhuwanupadhyay.schemas\", \"fields\": [ { \"name\": \"orderId\", \"type\": \"string\" }, { \"name\": \"paymentId\", \"type\": \"string\" } ]} After sometime, business need to add customerId attribute on PaymentRequested message payload for Order Service. To do so we need to define a second version of schema i.e. PaymentRequestedV2 and use it in the Order Service. PaymentRequestedV2 will publish by Order Service after upgrade. 1234567891011121314151617181920{ \"name\": \"PaymentRequestedV2\", \"type\": \"record\", \"namespace\": \"io.github.bhuwanupadhyay.schemas\", \"fields\": [ { \"name\": \"orderId\", \"type\": \"string\" }, { \"name\": \"orderAmount\", \"type\": \"string\" }, { \"name\": \"customerId\", \"type\": \"string\", \"default\": \"\" } ]} Order ServiceOnce schema upgraded for a message PaymentRequested then order service will publish the latest version i.e. V2 to their consumers: 12345678910111213141516171819202122@Slf4j@Service@EnableBinding(OrderEventSource.class)@RequiredArgsConstructorpublic class OrderEventPublisherService { private final OrderEventSource orderEventSource; @TransactionalEventListener // Attach it to the transaction of the repository operation public void handlePaymentRequestedEvent(PaymentRequestedEvent paymentRequestedEvent) { LOG.info(\"Handling event [PaymentRequestedEvent].\"); final PaymentRequestedV2 paymentRequested = PaymentRequestedV2.newBuilder() .setOrderId(paymentRequestedEvent.getOrderId().getOrderId()) .setOrderAmount(paymentRequestedEvent.getOrderAmount().asString()) .setCustomerId(paymentRequestedEvent.getCustomerId().getCustomerId()) .build(); orderEventSource .paymentRequested() .send(MessageBuilder.withPayload(paymentRequested).build()); // Publish the event LOG.info(\"Successfully published the event [PaymentRequestedV2].\"); }} Payment ServiceStill, payment service is consuming old version i.e. V1 for a message PaymentRequested: 1234567891011121314151617181920@Slf4j@Service@RequiredArgsConstructor@EnableBinding(PaymentEventSource.class) // Bind to the channel connection for the messagepublic class PaymentEventHandler { private final CreatePaymentCommandService createPaymentCommandService; // Listen to the stream of messages on the destination @StreamListener(target = PaymentEventSource.PAYMENT_REQUESTED_CHANNEL) public void receiveEvent(PaymentRequestedV1 paymentRequested) { LOG.info(\"Receive event [PaymentRequestedV1].\"); LOG.debug(\"Event payload {}.\", paymentRequested); final CreatePaymentCommand createPaymentCommand = new CreatePaymentCommand(); createPaymentCommand.setOrderId(paymentRequested.getOrderId().toString()); createPaymentCommand.setOrderAmount(paymentRequested.getOrderAmount().toString()); createPaymentCommandService.createPayment(createPaymentCommand); LOG.info(\"Successfully processed event [PaymentRequestedV1].\"); }} Run ExampleUse docker-compose.yaml to run necessary infrastructure for microservices. 1docker-compose up Clone Example Github Project in your directory. Build 1make pull &amp;&amp; make build Run Schema Registry – (On New Terminal) 1make pull &amp;&amp; make build Run Order Service – (On New Terminal) 1make order_service Run Payment Service – (On New Terminal) 1make payment_service Run Test - Perform following http request to test microservices. 123456789101112131415### Create New OrderPOST http://localhost:8080/ordersContent-Type: application/json{ \"itemId\": \"ITM00001\", \"quantity\": 20, \"customerId\": \"CUST00001\"}### Get ordersGET http://localhost:8080/orders### Get paymentsGET http://localhost:8081/payments ConclusionIn the above example, microservices were able to communicate between each other seamlessly even we applied schema changes on producer service but not on consumer service. Finally, we can exchange information between microservices safely by using schema registry and agree upon some sort of contracts between the microservice for message payloads. You can find example on github: Source Code document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/05/safely-exchanging-information-across-microservices/"},{"title":"Minikube Setup and Helm Deployment","text":"To run a Kubernetes cluster in your local machine and try our Kubernetes capabilities you can use Minikube. In this article, I will explain how to setup k8s command-line tools like kubectl minikube helm. Setup k8s tools kubectl1234567891011# Downloadcurl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl# Make the kubectl binary executablechmod +x ./kubectl# Move the binary in to your PATHsudo mv ./kubectl /usr/local/bin/kubectl# Verify commandkubectl version minikube12345678# Downloadcurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64# Move the binary into your PATHsudo install minikube-linux-amd64 /usr/local/bin/minikube# Verify commandminikube version helm12345# Installsudo snap install helm --classic# Verify commandhelm version Helm Deployment Create a helm project1helm create my-project Deploying using helm123456789minikube startkubectl config use-context minikubehelm dependency update my-projecthelm upgrade --install -f my-project/values.yaml deployment-name my-project --forcekubectl get pods Delete helm deployment1helm delete deployment-name Commands Useful Minikube commands12345678minikube status # See if Minikube is runningminikube start # Create and start Minikubeminikube dashboard # Access the Kubernetes dashboard running within the Minikube clusterminikube ssh # Login into the Minikube VMminikube addons list # Show the status of the available add-onsminikube stop # Stop Minikubeminikube delete # Delete the Minikube VMminikube ip # Show the Minikube IP Useful Helm commands123helm init # Initialize helmhelm create # Create helm charthelm install # Install helm deployment References https://kubernetes.io/docs/setup/learning-environment/minikube/ https://helm.sh/docs/intro/install/ https://helm.sh/docs/helm/ https://kubernetes.io/docs/tasks/tools/install-kubectl/ document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/06/minikube-setup-and-helm-deployment/"},{"title":"Expose spring boot microservice with ingress using helm","text":"In k8s deployment, ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. In this article, I will take you through how to expose spring boot microservice for the outside world in k8s deployment using ingress. This example needs kubectl minikube helm command-line tools in your machine. Simple Spring Boot MicroserviceLet’s create one simple spring boot microservice that just returns the given name. Initialize Project123456789NAME='Expose spring boot microservice with ingress using helm' &amp;&amp; PRJ=expose-spring-boot-microservice-with-ingress-using-helm &amp;&amp; \\mkdir -p $PRJ &amp;&amp; cd $PRJ &amp;&amp; \\curl https://start.spring.io/starter.tgz \\ -d dependencies=actuator,webflux \\ -d groupId=io.github.bhuwanupadhyay -d artifactId=$PRJ -d packageName=io.github.bhuwanupadhyay.example \\ -d applicationName=Spring Boot -d name=$NAME -d description=$NAME \\ -d language=kotlin -d platformVersion=2.3.1.RELEASE -d javaVersion=11 \\ -o demo.tgz &amp;&amp; \\ tar -xzvf demo.tgz &amp;&amp; rm -rf demo.tgz Simple API to return given name1234567891011@Configurationclass NameRoutes(private val handler: NameHandler) { @Bean fun router() = router { accept(APPLICATION_JSON).nest { GET(\"/names/{given-name}\", handler::findGivenName) } }} Containerizing Spring Boot ApplicationFrom Spring Boot 2.3.0.RELEASE the maven plugin of spring boot by default support build-image goal during execution which creates an OCI image using Cloud Native Buildpacks. 1234567891011121314&lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;build-image&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;imageName&gt;docker.io/bhuwanupadhyay/${project.artifactId}:${project.version}&lt;/imageName&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; Run mvn clean install : spring boot maven plugin will create a docker image. The end part of the output log: 123[INFO][INFO] Successfully built image 'docker.io/bhuwanupadhyay/expose-spring-boot-microservice-with-ingress-using-helm:0.0.1-SNAPSHOT'[INFO] To publish docker image in a registry run the following command 1docker push docker.io/bhuwanupadhyay/expose-spring-boot-microservice-with-ingress-using-helm:0.0.1-SNAPSHOT Helm ChartTo create a helm chart from your project directory run the following command. 1helm create src/microservice Replace value image repository and tag with your published docker image name and tag in src/microservice/values.yaml inside a helm chart. 1234image: repository: docker.io/bhuwanupadhyay/expose-spring-boot-microservice-with-ingress-using-helm pullPolicy: IfNotPresent tag: \"0.0.1-SNAPSHOT\" In your helm chart under src/microservice/templates/deployment.yaml change readinessProbe and livenessProbe health check settings, also modify container port to 8080 is the default for spring boot application. 1234567891011121314151617181920ports: - name: http containerPort: 8080 protocol: TCPlivenessProbe: httpGet: path: /actuator/health port: http initialDelaySeconds: 30 periodSeconds: 60 timeoutSeconds: 5 failureThreshold: 5readinessProbe: httpGet: path: /actuator/health port: http initialDelaySeconds: 30 periodSeconds: 5 timeoutSeconds: 5 failureThreshold: 5 Enable Ingress in HelmSimply modify file src/microservice/values.yaml accordingly. 123456789ingress: enabled: true annotations: kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" hosts: - host: microservice.minikube paths: - \"/\" DeploymentGet ready for the deployment! Start Minikube and Enable ingress1minikube start Minikube enable addons ingress and ingress-dns in12minikube addons enable ingressminikube addons enable ingress-dns Helm deployment123helm upgrade \\ --install -f src/microservice/values.yaml \\ example-deployment src/microservice --force Watch the deployment and ingress1watch kubectl get pods Modify /etc/hosts to add your host123456789# Know your host and address -&gt; Run the following commandkubectl get ingress# OutputNAME CLASS HOSTS ADDRESS PORTS AGEexample-deployment-microservice &lt;none&gt; microservice.minikube 172.17.0.2 80 63m# Add your host -&gt; Run the following commandsudo sed -i \"$ a 172.17.0.2 microservice.minikube\" /etc/hosts Test Microservice APISFirstly, install httpie command line tool. 1sudo apt install httpie Open New Terminal - Call microservice APIS 1234567# GET given namehttp http://microservice.minikube/names/k8sname# Output{ \"givenName\": \"k8sname\"} We are done ! Thanks for reading. Github References Versions: helm: v3.2.3 minikube: v1.11.0 kubectl: v1.17.0 https://docs.spring.io/initializr/docs/current/reference/html/ https://buildpacks.io/ https://docs.spring.io/spring-boot/docs/2.3.0.RELEASE/maven-plugin/reference/html/ document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/06/expose-spring-boot-microservice-with-ingress-using-helm/"},{"title":"Configmap in Spring Cloud Kubernetes","text":"For the integration of Spring Cloud and Spring Boot applications that are running inside Kubernetes, we can use Spring Cloud Kubernetes which provides Spring Cloud common interface implementations that are easy to use and ready for production. In k8s deployment, configmap can be used as a source for application configuration properties. In this article, I will take you through how to use spring cloud Kubernetes to provide configuration properties for spring boot applications in k8s deployment using congfigmap. This example needs kubectl minikube helm command-line tools in your machine. Create MicroserviceLet’s create one simple spring boot microservice that manages orders coming from a customer. In my example, order microservice exposes basic CRUD operations APIS, along with Spring Webflux I used Spring Data R2DBC for data access and Kotlin language. Initialize Project123456789NAME='Configmap in Spring Cloud Kubernetes' &amp;&amp; PRJ=configmap-in-spring-cloud-kubernetes &amp;&amp; \\mkdir -p $PRJ &amp;&amp; cd $PRJ &amp;&amp; \\curl https://start.spring.io/starter.tgz \\ -d dependencies=actuator,webflux,cloud-starter,data-r2dbc,h2,postgresql \\ -d groupId=io.github.bhuwanupadhyay -d artifactId=$PRJ -d packageName=io.github.bhuwanupadhyay.example \\ -d applicationName=Spring Boot -d name=$NAME -d description=$NAME \\ -d language=kotlin -d platformVersion=2.3.1.RELEASE -d javaVersion=11 \\ -o demo.tgz &amp;&amp; \\ tar -xzvf demo.tgz &amp;&amp; rm -rf demo.tgz Basic CRUD operations APIS12345678910111213141516@Table(\"ORDERS\")data class OrderEntity(@Id var id: Long?, var item: String, var quantity: Int)@Configurationclass OrderRoutes(private val handler: OrderHandler) { @Bean fun router() = router { accept(APPLICATION_JSON).nest { POST(\"/orders\", handler::save) GET(\"/orders\", handler::findAll) GET(\"/orders/{id}\", handler::findOne) PUT(\"/orders/{id}\", handler::update) } }} Containerizing Spring Boot ApplicationFrom Spring Boot 2.3.0.RELEASE the maven plugin of spring boot by default support build-image goal during execution which creates an OCI image using Cloud Native Buildpacks. 1234567891011121314&lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;build-image&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;imageName&gt;docker.io/bhuwanupadhyay/${project.artifactId}:${project.version}&lt;/imageName&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; Run mvn clean install : spring boot maven plugin will create a docker image. The end part of the output log: 123[INFO] [INFO] Successfully built image 'docker.io/bhuwanupadhyay/configmap-in-spring-cloud-kubernetes:0.0.1-SNAPSHOT'[INFO] To publish docker image in the registry run the following command 1docker push docker.io/bhuwanupadhyay/configmap-in-spring-cloud-kubernetes:0.0.1-SNAPSHOT Helm ChartTo create a helm chart from your project directory run the following command. 1helm create src/helm-chart Replace value image repository and tag with your published docker image name and tag in src/helm-chart/values.yaml inside the helm chart. 1234image: repository: docker.io/bhuwanupadhyay/configmap-in-spring-cloud-kubernetes pullPolicy: IfNotPresent tag: \"0.0.1-SNAPSHOT\" Order microservice required data source configuration properties in the deployment to connect with the database to store order information. Let’s say we have two spring profiles dev and prod. In dev profile we want to run an application using h2 database while prod profile we want to run an application using postgresql database. H2 is an embedded database so no need to create different pod instance, but it is not the case for postgresql because it is not embeddable, so we need to run in a different pod. To run postgresql in helm deployment you need to add dependency inside helm chart src/helm-chart/Chart.yaml. 12345dependencies: - name: postgresql alias: Spring Bootdb version: 8.10.5 repository: https://charts.bitnami.com/bitnami In Kubernetes config map is used to provide application configuration properties. To add configmap in your helm chart simply create a new YAML file in src/helm-chart/templates/configmap.yaml. For our example, we have to define multiple spring profiles for h2 and postgresql that are given below: 1234567891011121314151617181920212223242526272829303132apiVersion: v1kind: ConfigMapmetadata: name: {{ include \"helm-chart.fullname\" . }}-config labels: {{- include \"helm-chart.labels\" . | nindent 4 }}data: application.yaml: |- management: endpoints: web: base-path: /actuator exposure: include: ['configprops', 'env', 'health', 'info', 'logfile', 'loggers', 'threaddump'] endpoint: health: show-details: always spring: application: name: order-service --- spring: profiles: dev r2dbc: url: r2dbc:h2:mem://test?options=DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE --- spring: profiles: prod r2dbc: url: r2dbc:postgresql://{{ .Release.Name }}-Spring Bootdb:5432/{{ .Values.Spring Bootdb.postgresqlDatabase }} username: {{ .Values.Spring Bootdb.postgresqlUsername }} password: {{ .Values.Spring Bootdb.postgresqlPassword }} Spring Cloud KubernetesAdd dependency that used to load application properties from Kubernetes ConfigMaps and Secrets or reload application properties when a ConfigMap or Secret changes. 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-kubernetes-config&lt;/artifactId&gt;&lt;/dependency&gt; You need to make sure a pod that runs with spring-cloud-Kubernetes has access to the Kubernetes API. Using helm you can create a service account with Role and RoleBinding that has access to read configmaps and secrets by defining new yaml file in src/helm-chart/templates/cluster-reader.yaml. 12345678910111213141516171819202122232425kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: {{ include \"helm-chart.fullname\" . }}-reader labels: {{- include \"helm-chart.labels\" . | nindent 4 }}rules: - apiGroups: [\"\", \"extensions\", \"apps\"] resources: [\"configmaps\", \"pods\", \"services\", \"endpoints\", \"secrets\"] verbs: [\"get\", \"list\", \"watch\"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: {{ include \"helm-chart.fullname\" . }} labels: {{- include \"helm-chart.labels\" . | nindent 4 }}roleRef: kind: Role name: {{ include \"helm-chart.fullname\" . }}-reader apiGroup: \"\"subjects: - kind: ServiceAccount name: {{ include \"helm-chart.fullname\" . }} apiGroup: \"\" Finally, you need to provide environment variables to tell the application to use your configmap and namespace for spring cloud Kubernetes. Also, In your helm chart under src/microservice/templates/deployment.yaml change env, readinessProbe and livenessProbe health check settings, also modify container port to 8080 is the default for spring boot application. 123456789101112131415161718192021222324252627env: - name: SPRING_PROFILES_ACTIVE value: {{ .Values.spring.profiles.active }} - name: SPRING_CLOUD_KUBERNETES_CONFIG_NAME value: {{ include \"helm-chart.fullname\" . }}-config - name: SPRING_CLOUD_KUBERNETES_CONFIG_NAMESPACE value: {{ .Release.Namespace }}ports: - name: http containerPort: 8080 protocol: TCPlivenessProbe: httpGet: path: /actuator/health port: http initialDelaySeconds: 30 periodSeconds: 60 timeoutSeconds: 5 failureThreshold: 5readinessProbe: httpGet: path: /actuator/health port: http initialDelaySeconds: 30 periodSeconds: 5 timeoutSeconds: 5 failureThreshold: 5 DeploymentGet ready for the deployment! Start Minikube1minikube start Add helm repositories1helm repo add bitnami https://charts.bitnami.com/bitnami Update charts1helm dependency update src/helm-chart DEV Profile helm deployment12345helm upgrade \\ --install -f src/helm-chart/values.yaml \\ --set spring.profiles.active=dev \\ --set Spring Bootdb.enabled=false \\ example-deployment src/helm-chart --force PROD Profile helm deployment123456789helm upgrade \\ --install -f src/helm-chart/values.yaml \\ --set spring.profiles.active=prod \\ --set Spring Bootdb.enabled=true \\ --set Spring Bootdb.postgresqlDatabase=orders-db \\ --set Spring Bootdb.postgresqlUsername=user \\ --set Spring Bootdb.postgresqlPassword=password \\ --set Spring Bootdb.persistence.enabled=false \\ example-deployment src/helm-chart --force Watch the deployment1watch kubectl get pods Test Order Microservice APISFirstly, install httpie command line tool. 1sudo apt install httpie Open New Terminal - Port forward for order microservice 1234567891011# Get pods -&gt; Run the following commandkubectl get pods# OutputNAME READY STATUS RESTARTS AGEexample-deployment-helm-chart-8ff55d4db-rnt6j 1/1 Running 0 12mexample-deployment-Spring Bootdb-0 1/1 Running 0 12m# Port forward -&gt; Run the following commandkubectl port-forward example-deployment-helm-chart-8ff55d4db-rnt6j 8080:8080 Open New Terminal - Call order service APIS 123456# POST ordersecho '{\"item\": \"k8s-item\", \"quantity\": 20}' | http POST :8080/orders# GET ordershttp :8080/orders We are done ! Thanks for reading. Github References Versions: helm - v3.2.3 minikube - v1.11.0 kubectl - v1.17.0 https://docs.spring.io/initializr/docs/current/reference/html/ https://buildpacks.io/ https://docs.spring.io/spring-boot/docs/2.3.0.RELEASE/maven-plugin/reference/html/ https://spring.io/projects/spring-cloud-kubernetes document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/06/configmap-in-spring-cloud-kubernetes/"},{"title":"Semantic versioning on docker build and helm chart","text":"Helm best practice guide advocate semantic versioning for the helm chart that your release for deployment. Wherever possible, Helm uses SemVer 2 to represent version numbers. Semantic versioning is a meaningful method for incrementing version numbers. So, today we will explore how to release helm charts and docker build by using semantic versioning convention. In this example, I will release semantic versions for helm chart and docker image of spring boot microservice that build upon maven. Docker build and Helm chart for spring bootLet’s start with spring boot a simple microservice that exposes API to return a given name. Initialize Project123456789NAME='Semantic versioning on docker build and helm chart' &amp;&amp; PRJ=semantic-versioning-on-docker-build-and-helm-chart &amp;&amp; \\mkdir -p $PRJ &amp;&amp; cd $PRJ &amp;&amp; \\curl https://start.spring.io/starter.tgz \\ -d dependencies=actuator,webflux \\ -d groupId=io.github.bhuwanupadhyay -d artifactId=$PRJ -d packageName=io.github.bhuwanupadhyay.example \\ -d applicationName=Spring Boot -d name=$NAME -d description=$NAME \\ -d language=kotlin -d platformVersion=2.3.1.RELEASE -d javaVersion=11 \\ -o demo.tgz &amp;&amp; \\ tar -xzvf demo.tgz &amp;&amp; rm -rf demo.tgz Create API to return given name1234567891011@Configurationclass NameRoutes(private val handler: NameHandler) { @Bean fun router() = router { accept(APPLICATION_JSON).nest { GET(\"/names/{given-name}\", handler::findGivenName) } }} Dockerfile for spring boot src/main/docker/DockerfileFrom Spring Boot 2.3.0.RELEASE they introduced layertools to create optimized Docker images that can be built with a dockerfile. 12345678910111213FROM adoptopenjdk:11.0.7_10-jre-hotspot as builderWORKDIR /appARG JAR_FILE=target/*.jarCOPY ${JAR_FILE} app.jarRUN java -Djarmode=layertools -jar app.jar extractFROM adoptopenjdk:11.0.7_10-jre-hotspotWORKDIR /appCOPY --from=builder app/dependencies/ ./COPY --from=builder app/spring-boot-loader/ ./COPY --from=builder app/snapshot-dependencies/ ./COPY --from=builder app/application/ ./ENTRYPOINT [\"java\", \"org.springframework.boot.loader.JarLauncher\"] Helm chart for spring boot src/main/helm/my-serviceSimply run the create helm command. 1mkdir -p src/main/helm &amp;&amp; helm create src/main/helm/my-service In your helm chart under src/main/helm/my-service/templates/deployment.yaml change readinessProbe and livenessProbe health check settings, also modify container port to 8080 is the default for spring boot application. 1234567891011121314151617181920ports: - name: http containerPort: 8080 protocol: TCPlivenessProbe: httpGet: path: /actuator/health port: http initialDelaySeconds: 30 periodSeconds: 60 timeoutSeconds: 5 failureThreshold: 5readinessProbe: httpGet: path: /actuator/health port: http initialDelaySeconds: 30 periodSeconds: 5 timeoutSeconds: 5 failureThreshold: 5 Also, replace value image repository with your published docker image name without a tag in src/main/helm/my-service/values.yaml inside a helm chart. 1234image: repository: docker.io/bhuwanupadhyay/my-service pullPolicy: IfNotPresent tag: \"\" In src/main/helm/my-service/Chart.yaml there are two properties: version [chart version] - Versions are expected to follow Semantic Versioning (https://semver.org/) This is the chart version. This version number should be incremented each time you make changes. appVersion [default value for image tag] - Versions are expected to follow Semantic Versioning (https://semver.org/) This is the version number of the application being deployed. This version number should be incremented each time you make changes to the application. In maven pom.xml, the flatten-maven-plugin to set revision number for project, which will use by dockerfile-maven-plugin to build the docker image repository/image-name:&lt;revision&gt; with revision and helm-maven-plugin create package with that revision for chart version and appVersion. It’s very important to provide consistent releases during the life cycle of the product. To achieve this we will use very popular tool Semantic Release with Conventional Commits. Semantic Release Process Github PipelineLet’s create semantic-release configuration .releaserc file in your project directory. In this configuration, we have two commands given by @semantic-release/exec plugin that we will use to a build the release and publish helm package on GitHub Packages and docker image on docker.io. prepareCmd: The shell command to execute during the prepare step. publishCmd: The shell command to execute during the publish step. 1234567891011121314{ \"branches\": [\"master\"], \"plugins\": [ \"@semantic-release/commit-analyzer\", \"@semantic-release/release-notes-generator\", \"@semantic-release/github\", \"@semantic-release/git\", \"@semantic-release/changelog\", [\"@semantic-release/exec\", { \"prepareCmd\" : \"./bot.sh --prepare ${nextRelease.version}\", \"publishCmd\" : \"./bot.sh --publish ${nextRelease.version}\" }] ]} Here bot.sh is a script file used to build and publish the docker image and helm chart package using the next version given by a semantic release process. 12345678910111213141516171819202122232425262728293031set -eoption=\"${1}\"default_version='0.0.0-SNAPSHOT'next_version=\"${2:-$default_version}\"case ${option} in --prepare) ./mvnw \\ -B -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn -V \\ clean install -Drevision=\"$next_version\" ;; --publish) # Publish Docker in Github Packages DOCKER_PKG=docker.pkg.github.com/bhuwanupadhyay/semantic-versioning-on-docker-build-and-helm-chart/my-service:\"$next_version\" docker login docker.pkg.github.com -u BhuwanUpadhyay -p \"$GITHUB_TOKEN\" docker tag docker.io/bhuwanupadhyay/my-service:\"$next_version\" \"$DOCKER_PKG\" docker push \"$DOCKER_PKG\" # Publish Helm chart in Github Releases HELM_CHART=\"my-service-$next_version.tgz\" HELM_CHART_FILE_PATH=\"$(pwd)/target/helm/repo/$FILE_NAME\" # TODO: yourself # Write a suitable script to upload your helm chart in your Artifactory or chart museum. echo \"Mock publish: $HELM_CHART from $HELM_CHART_FILE_PATH\" ;; *) echo \"`basename ${0}`:usage: [--prepare] | [--publish]\" exit 1 # Command to come out of the program with status 1 ;;esac Finally, we need a workflow action YAML configuration to run the Github pipeline under .github/workflows/build.yml. 123456789101112131415161718192021222324252627282930name: Java CIon: push: branches: - '*'jobs: build: runs-on: ubuntu-latest name: Build steps: - uses: actions/checkout@v2 - name: Set up JDK 11 uses: actions/setup-java@v1 with: java-version: 11 - name: Build run: ./bot.sh --prepare - uses: actions/setup-node@v1 name: Install semantic-release if: github.ref == 'refs/heads/master' &amp;&amp; github.event_name == 'push' with: node-version: \"12.x\" - name: Release if: github.ref == 'refs/heads/master' &amp;&amp; github.event_name == 'push' run: | npm install -g semantic-release @semantic-release/{git,changelog,exec} semantic-release env: GITHUB_TOKEN: ${{ github.token }} We are done ! Thanks for reading. Github References https://docs.spring.io/spring-boot/docs/2.3.0.RELEASE/reference/htmlsingle/#layering-docker-images document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/06/semantic-versioning-on-docker-build-and-helm-chart/"},{"title":"Spring Boot Docker Containerization","text":"Spring Boot is one of the very popular framework to build the microservices and the docker container is the default choice to run the application in a cloud-native environment. Docker provides the ability to package and run an application in a loosely isolated environment called a container. So, it’s very important to build the right layers of the docker image for your application. This blog post shows the available options to build a docker image for the spring boot application. Before deep into how to build the docker image, Let’s create one very simple spring boot application that will return the given name as a response. After that, we will explore how to build a docker image of this application. Create a Spring Boot applicationTo create a Spring Boot application, we’ll use Spring Initializr. The application that we’ll create uses: Spring Boot Spring WebFlux Spring Actuator Kotlin Initialize Project12345678910NAME='Spring Boot Docker Containerization' &amp;&amp; \\PRJ=spring-boot-docker-containerization &amp;&amp; \\mkdir -p $PRJ &amp;&amp; cd $PRJ &amp;&amp; \\curl https://start.spring.io/starter.tgz \\ -d dependencies=actuator,webflux \\ -d groupId=io.github.bhuwanupadhyay -d artifactId=$PRJ \\ -d packageName=io.github.bhuwanupadhyay.example \\ -d applicationName=Spring Boot -d name=\"$NAME\" -d description=\"$NAME\" \\ -d language=java -d platformVersion=2.3.1.RELEASE -d javaVersion=11 \\ -o demo.tgz &amp;&amp; tar -xzvf demo.tgz &amp;&amp; rm -rf demo.tgz API ExampleCreate NameManager.kt under package io.github.bhuwanupadhyay.example and add the following text: 12345678910111213141516171819202122@Componentclass NameHandler { fun findGivenName(req: ServerRequest): Mono&lt;ServerResponse&gt; { return Optional.ofNullable(req.pathVariable(\"given-name\")) .map { t -&gt; ok().bodyValue(\"{ \\\"givenName\\\": \\\"$t\\\"}\") } .orElseGet { badRequest().build() } }}@Configurationclass NameRoutes(private val handler: NameHandler) { @Bean fun router() = router { accept(APPLICATION_JSON).nest { GET(\"/names/{given-name}\", handler::findGivenName) } }} Docker ContainerizationThere are four ways to containerize spring boot application. Let’s take a look one by one. Using fat jarDockerfile to create a docker image of spring boot application with a fat jar. 1234FROM amd64/openjdk:14-alpineARG JAR_FILE=target/*.jarCOPY ${JAR_FILE} app.jarENTRYPOINT [\"java\", \"-jar\" , \"/app.jar\"] A maven profile to build a docker image with plugins: spring-boot-maven-plugin and dockerfile-maven-plugin. 123456789101112131415161718192021222324252627282930313233343536&lt;profile&gt; &lt;id&gt;fatJar&lt;/id&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;dockerfile-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.4.13&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;default&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;build&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;repository&gt;docker.io/bhuwanupadhyay/${project.artifactId}-fat-jar&lt;/repository&gt; &lt;dockerfile&gt;${project.basedir}/src/main/docker/fat-jar.dockerfile&lt;/dockerfile&gt; &lt;tag&gt;${project.version}&lt;/tag&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/profile&gt; To build and test run the following command: 1234567891011# Build a docker imagemvn clean install -PfatJar# Run appdocker run -d -p8080:8080 docker.io/bhuwanupadhyay/spring-boot-docker-containerization-fat-jar:0.0.1-SNAPSHOT# Test APIcurl http://localhost:8080/names/hurry# Output{ \"givenName\": \"hurry\"} Using classpath in exploded jarDockerfile to create a docker image of spring boot application with an exploded jar. 1234567891011121314# Stage 0, \"builder\", extract fat jarFROM amd64/openjdk:14-alpine as builderARG JAR_FILE=target/*.jarCOPY ${JAR_FILE} /target/app.jarRUN mkdir -p /target/dependency &amp;&amp; (cd /target/dependency; jar -xf ../*.jar)# Stage 1, \"boot-app\"FROM amd64/openjdk:14-alpineRUN addgroup -S spring &amp;&amp; adduser -S spring -G springUSER spring:springCOPY --from=builder /target/dependency/BOOT-INF/lib /app/libCOPY --from=builder /target/dependency/BOOT-INF/classes /appCOPY --from=builder /target/dependency/META-INF /appENTRYPOINT [\"java\", \"-cp\" , \"app:app/lib/*\", \"io.github.bhuwanupadhyay.example.Spring Boot\"] A maven profile to build a docker image with plugins: spring-boot-maven-plugin and dockerfile-maven-plugin. 123456789101112131415161718192021222324252627282930313233343536&lt;profile&gt; &lt;id&gt;flatClasspath&lt;/id&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;dockerfile-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.4.13&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;default&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;build&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;repository&gt;docker.io/bhuwanupadhyay/${project.artifactId}-flat-classpath&lt;/repository&gt; &lt;dockerfile&gt;${project.basedir}/src/main/docker/flat-classpath.dockerfile&lt;/dockerfile&gt; &lt;tag&gt;${project.version}&lt;/tag&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/profile&gt; To build and test run the following command: 1234567891011# Build a docker imagemvn clean install -PflatClasspath# Run appdocker run -d -p8081:8080 docker.io/bhuwanupadhyay/spring-boot-docker-containerization-flat-classpath:0.0.1-SNAPSHOT# Test APIcurl http://localhost:8081/names/hurry# Output{ \"givenName\": \"hurry\"} Using layertoolsDockerfile to create a docker image of spring boot application with a layertools. 12345678910111213FROM adoptopenjdk:11.0.7_10-jre-hotspot as builderWORKDIR /appARG JAR_FILE=target/*.jarCOPY ${JAR_FILE} app.jarRUN java -Djarmode=layertools -jar app.jar extractFROM adoptopenjdk:11.0.7_10-jre-hotspotWORKDIR /appCOPY --from=builder app/dependencies/ ./COPY --from=builder app/spring-boot-loader/ ./COPY --from=builder app/snapshot-dependencies/ ./COPY --from=builder app/application/ ./ENTRYPOINT [\"java\", \"org.springframework.boot.loader.JarLauncher\"] A maven profile to build a docker image with plugins: spring-boot-maven-plugin and dockerfile-maven-plugin. 12345678910111213141516171819202122232425262728293031323334&lt;profile&gt; &lt;id&gt;layertools&lt;/id&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;layers&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/layers&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;dockerfile-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.4.13&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;default&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;build&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;repository&gt;docker.io/bhuwanupadhyay/${project.artifactId}-layertools&lt;/repository&gt; &lt;dockerfile&gt;${project.basedir}/src/main/docker/layertools.dockerfile&lt;/dockerfile&gt; &lt;tag&gt;${project.version}&lt;/tag&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/profile&gt; To build and test run the following command: 1234567891011# Build a docker imagemvn clean install -Playertools# Run appdocker run -d -p8082:8080 docker.io/bhuwanupadhyay/spring-boot-docker-containerization-layertools:0.0.1-SNAPSHOT# Test APIcurl http://localhost:8082/names/hurry# Output{ \"givenName\": \"hurry\"} Using Buildpacks.A maven profile to build docker image with plugins: spring-boot-maven-plugin. 123456789101112131415161718192021&lt;profile&gt; &lt;id&gt;buildpacks&lt;/id&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;build-image&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;imageName&gt;docker.io/bhuwanupadhyay/${project.artifactId}-buildpacks:${project.version}&lt;/imageName&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/profile&gt; To build and test run the following command: 1234567891011# Build a docker imagemvn clean install -Pbuildpacks# Run appdocker run -d -p8083:8080 docker.io/bhuwanupadhyay/spring-boot-docker-containerization-buildpacks:0.0.1-SNAPSHOT# Test APIcurl http://localhost:8083/names/hurry# Output{ \"givenName\": \"hurry\"} We are done, Thanks for reading! Github References https://docs.spring.io/spring-boot/docs/2.3.0.RELEASE/reference/htmlsingle https://github.com/spotify/dockerfile-maven document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/06/spring-boot-docker-containerization/"},{"title":"SSL in spring boot application","text":"How to enable SSL in Spring Boot? Secure Sockets Layer, is an encryption-based Internet security protocol. The primary reason why SSL is used is to keep sensitive information sent across the Internet encrypted so that only the intended recipient can access it. This is important because the information you send on the Internet is passed from computer to computer to get to the destination server. Any computer in between you and the server can see your credit card numbers, usernames and passwords, and other sensitive information if it is not encrypted with an SSL certificate. When an SSL certificate is used, the information becomes unreadable to everyone except for the server you are sending the information to. This protects it from hackers and identity thieves. -« sslshopper.com »- How to hack SSL less Spring Boot Application?Let’s create a simple spring boot application that will have a POST method to create an order and GET method to get orders. Run the following to create order microservice spring boot application: 1curl https://start.spring.io/starter.tgz -d dependencies=actuator,webflux -d language=java -d platformVersion=2.3.1.RELEASE -d javaVersion=14 -d baseDir=ssl-in-spring-boot-application | tar -xzvf - Create OrderController.java and add the following text: 12345678910111213141516171819202122232425record Order(@JsonProperty(\"item\")String item, @JsonProperty(\"quantity\")Integer quantity) { public Order { if (quantity &lt; 1) { throw new IllegalArgumentException(\"Quantity should be positive number.\"); } }}@RestControllerpublic class OrderController { private final List&lt;Order&gt; orders = new ArrayList&lt;&gt;(); @PostMapping(\"/orders\") public Order create(@RequestBody final Order request) { this.orders.add(request); return this.orders.get(this.orders.size() - 1); } @GetMapping(\"/orders\") public List&lt;Order&gt; findAll() { return this.orders; }} To start the application simply run the command: mvn spring-boot:run and verify with the command: http :8080/actuator/health. Hack Time! Wireshark is the widely-used network protocol analyzer. It lets you see what’s happening on your network at a microscopic level. -« wireshark.org »- Go through the following command and steps to install Wireshark in ubuntu. 123456789101112# 1. Add ppa repository and installsudo add-apt-repository ppa:wireshark-dev/stablesudo apt-get updatesudo apt-get install wireshark# 2. Should non-superusers be able to capture packets? - Yes# 3. Give permissionsudo chmod +x /usr/bin/dumpcap# 4. Logout and login again# 5. Launch wiresharkwireshark In the Wireshark window select Loopback: lo interface, right-click on it, and hit Start capture option. As a result, Wireshark started to capture all network packets but we need data that is transferring between the Spring Boot server (in port 8080) and httpie client to get it we have to filter tcp.port == 8080 and hit enter. Now, let’s make POST http request on localhost 8080. 1echo '{\"item\": \"hack-item\", \"quantity\": 20}' | http POST :8080/orders Go back to Wireshark you will see a list of packets that is captured when you make this request. Among them, select POST /orders, right-click on it and select Follow -&gt; TCP Stream and you will see all values that were sent by the client on this request. The output looks like below: 123456789101112131415POST /orders HTTP/1.1Host: localhost:8080User-Agent: HTTPie/0.9.8Accept-Encoding: gzip, deflateAccept: application/json, */*Connection: keep-aliveContent-Type: application/jsonContent-Length: 38{\"item\": \"hack-item\", \"quantity\": 20}HTTP/1.1 200 OKContent-Type: application/jsonContent-Length: 34{\"item\":\"hack-item\",\"quantity\":20} SSL in Spring BootSpring Boot provides a set of a declarative server.ssl.* properties. We can use those properties to configure HTTPS. Self-signed CertificateThe tools like OpenSSL, Keytool, etc. helps to generate a self-signed certificate. For example, to generate a key store (key bag) with keytool: 1234567keytool -genkey \\ -alias Spring Boot \\ -storetype PKCS12 \\ -keyalg RSA \\ -keysize 2048 \\ -keystore keystore.p12 \\ -validity 4000 Here, we store the keys using PKCS #12 which is like a collection of keys such as private keys and public key certifications. We can look inside our generated keystore (keystore.p12) with OpenSSL. 1openssl pkcs12 -info -in keystore.p12 Enable HTTPSYou should copy your generated keystore.p12 and create new application-ssl.properties file in your Spring boot application resources with follwing text: 12345server.port=8443server.ssl.key-store=classpath:keystore.p12server.ssl.key-store-password=mypassserver.ssl.keyStoreType=PKCS12server.ssl.keyAlias=Spring Boot Again, you need to start the application for this simply run the command: mvn spring-boot:run -Dspring-boot.run.profiles=ssl and verify with the command: http --verify=no https://localhost:8443/actuator/health. Hack Time! This time we have to add a filter in Wireshark tcp.port == 8443 and hit enter. Now, let’s make POST http request on localhost 8080. 1echo '{\"item\": \"hack-item\", \"quantity\": 20}' | http --verify=no POST https://localhost:8443/orders Go back to Wireshark you will see a list of packets that is captured when you make this request. Among them select Application Data, right-click on it, and select Follow -&gt; TCP Stream and you will see encrypted values that were sent by the client on this request. The output looks like below: 1234...........XP4..1.'.....U....... X..U.w:.. .b.e...\"....G...\".........M.z..J.V.......,.0.+./...................$.(.#.'.... ...........k.g.9.3.............=.&lt;.5./.....]........ localhost................. &lt;more&gt;.... That’s all folks.Thanks for reading! Code example on Github. References https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#howto-configure-ssl https://www.sslshopper.com/why-ssl-the-purpose-of-using-ssl-certificates.html https://www.youtube.com/watch?v=r0l_54thSYU https://wiki.wireshark.org/CaptureSetup/Loopback document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/07/ssl-in-spring-boot-application/"},{"title":"Highlight of Jdk 15 Features","text":"Highlighting new features in Java 15 are sealed types, local types. Sealed Types Sealed classes and interfaces restrict which other classes or interfaces may extend or implement them. Goals Allow the author of a class or interface to control which code is responsible for implementing it. Provide a more declarative way than access modifiers to restrict the use of a superclass. Example of sealed classes: 1234567891011121314151617181920212223242526272829303132333435363738394041sealed abstract class Transaction permits DebitTransaction, CreditTransaction { static final BigDecimal TEN = BigDecimal.valueOf(10); static final BigDecimal NINETY = BigDecimal.valueOf(90); static final BigDecimal HUNDRED = BigDecimal.valueOf(100); static final BigDecimal THOUSANDS = BigDecimal.valueOf(1000); static final BigDecimal ONE_THOUSANDS_ONE = BigDecimal.valueOf(1001); protected final BigDecimal value; Transaction(BigDecimal value) { this.value = value; } abstract BigDecimal entry(BigDecimal amount);}final class CreditTransaction extends Transaction { public CreditTransaction(BigDecimal baseAmount) { super(baseAmount); } @Override public BigDecimal entry(BigDecimal amount) { return this.value.add(amount); }}final class DebitTransaction extends Transaction { public DebitTransaction(BigDecimal baseAmount) { super(baseAmount); } @Override public BigDecimal entry(BigDecimal amount) { return this.value.subtract(amount); }} Example of sealed interfaces: 1234567891011121314151617181920212223242526272829303132333435363738394041424344sealed interface Shape permits Circle, Rectangle { long area();}final class Circle implements Shape { private final int radius; Circle(int radius) { this.radius = radius; } @Override public long area() { return Math.round(3.14 * radius * radius); }}non-sealed class Rectangle implements Shape { private final int length; private final int width; Rectangle(int length, int width) { this.length = length; this.width = width; } @Override public long area() { return length * width; }}final class Square extends Rectangle { Square(int side) { super(side, side); }} Local TypesJava 15 now allows us to create an enums, interfaces and records inside a local method. Example of local record: 12345678910111213141516171819public List&lt;Customer&gt; filterForGoldCustomer(List&lt;Customer&gt; customers) { record GoldCustomer(Customer customer, List&lt;Order&gt; orders) { boolean hasEnoughOrders() { boolean hasMoreThanOrEqualToTwo = GoldCustomer.this.orders.size() &gt;= 2; Integer totalQuantity = GoldCustomer.this.orders.stream().map(Order::getQuantity).reduce(0, Integer::sum); return hasMoreThanOrEqualToTwo &amp;&amp; totalQuantity &gt; 100; } } return customers.stream() .map(customer -&gt; new GoldCustomer(customer, customer.getOrders())) .filter(GoldCustomer::hasEnoughOrders) .map(GoldCustomer::customer) .collect(Collectors.toList());} Example of local enums: 1234567891011121314public List&lt;Customer&gt; filterActiveOrVipCustomer(List&lt;Customer&gt; customers) { enum Status { Active, Inactive, Suspended, Vip; public static boolean isActiveOrVip(String status) { return Objects.equals(status, Active.name()) || Objects.equals(status, Vip.name()); } } return customers.stream() .filter(customer -&gt; Status.isActiveOrVip(customer.getStatus())) .collect(Collectors.toList());} Example of local interfaces: 1234567891011121314151617181920public List&lt;Customer&gt; filterInactiveCustomer(List&lt;Customer&gt; customers) { interface InactiveCollector { List&lt;Customer&gt; inactive(List&lt;Customer&gt; customers); } class CustomerInactiveCollector implements InactiveCollector { @Override public List&lt;Customer&gt; inactive(List&lt;Customer&gt; customers) { return customers.stream().filter(customer -&gt; Objects.equals(customer.getStatus(), \"Inactive\")).collect(Collectors.toList()); } } InactiveCollector inactiveCollector = new CustomerInactiveCollector(); return inactiveCollector.inactive(customers);} That’s all folks.Thanks for reading! References https://openjdk.java.net/projects/jdk/15/ document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/08/highlight-of-jdk-15-features/"},{"title":"Error Handler with RestTemplate","text":"In this code snippet, I will show how to implement and inject the ResponseErrorHandler interface in a RestTemplate instance - to gracefully handle HTTP errors returned by remote APIs. The second law of thermodynamics, in principle, states that a closed system’s disorder cannot be reduced, it can only remain unchanged or increase. A measure of this disorder is entropy. This law also seems plausible for software systems; as a system is modified, its disorder, or entropy, tends to increase. This is known as software entropy. - wikipedia 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class ClientRestException extends RuntimeException { public ClientRestException(String message) { super(message); }}class ClientRestTemplateErrorHandler extends DefaultResponseErrorHandler { @Override public void handleError(ClientHttpResponse response) throws IOException { if (response.getStatusCode().is4xxClientError() || response.getStatusCode().is5xxServerError()) { try (BufferedReader reader = new BufferedReader(new InputStreamReader(response.getBody()))) { String httpBodyResponse = reader.lines().collect(Collectors.joining(\"\")); throw new ClientRestException(httpBodyResponse); } } }}record OrderInfo(@JsonProperty(\"customerId\") String customerId, @JsonProperty(\"itemId\") String itemId, @JsonProperty(\"quantity\") Integer quantity) {}@Serviceclass OrderServiceClient { private final String orderServiceUrl; private final RestTemplate restTemplate; public OrderServiceClient(@Value(\"${order-service.url:http://localhost:8080}\") String orderServiceUrl) { this.orderServiceUrl = orderServiceUrl; this.restTemplate = new RestTemplate(); this.restTemplate.setErrorHandler(new ClientRestTemplateErrorHandler()); } public OrderInfo getOrderInfo(String orderId) { URI uri = UriComponentsBuilder.fromHttpUrl(orderServiceUrl) .path(\"/orders/{orderId}\").queryParam(\"expired\", \"NO\").build(orderId); RequestEntity&lt;Void&gt; getByOrderId = RequestEntity.get(uri).build(); return restTemplate.exchange(getByOrderId, OrderInfo.class).getBody(); }}@Spring BootApplicationpublic class ErrorHandlerWithRestTemplate { private final OrderServiceClient orderServiceClient; public ErrorHandlerWithRestTemplate(OrderServiceClient orderServiceClient) { this.orderServiceClient = orderServiceClient; } public static void main(String[] args) { SpringApplication.run(ErrorHandlerWithRestTemplate.class, args); } @EventListener public void on(ApplicationReadyEvent event) { orderServiceClient.getOrderInfo(\"123\"); }} document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/08/error-handler-with-resttemplate/"},{"title":"Enforce onion architecture in your code","text":"ArchUnit’s main focus is to automatically test architecture and coding rules. In this code snippet, I will show how to use ArchUnit to enforce architecture in your code. To create something exceptional, your mindset must be relentlessly focused on the smallest detail. - Giorgio Armani 1234567&lt;dependency&gt; &lt;groupId&gt;com.tngtech.archunit&lt;/groupId&gt; &lt;artifactId&gt;archunit-junit5&lt;/artifactId&gt; &lt;!-- &lt;artifactId&gt;archunit-junit4&lt;/artifactId&gt; --&gt; &lt;version&gt;0.14.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; Bounded Context-Package Artifacts ArchUnit Test123456789101112131415161718192021@AnalyzeClasses(packages = CodingRuleTest.PACKAGE)class CodingRuleTest { public static final String PACKAGE = \"io.retailstore.cart\"; @ArchTest private final ArchRule classes_are_under_packages = ArchRuleDefinition.classes() .should() .resideInAnyPackage(PACKAGE, \"..application..\", \"..domain..\", \"..infrastructure..\", \"..interfaces..\"); @ArchTest private final ArchRule onion_dependencies_are_respected = Architectures .onionArchitecture() .domainModels(\"..domain.model..\") .domainServices(\"..domain.services..\") .applicationServices(\"..application.commandservices..\", \"..application.queryservices..\") .adapter(\"outbound\", \"..infrastructure..\", \"..application.outboundservices..\") .adapter(\"inbound\", \"..interfaces..\");} document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/08/enforce-onion-architecture-in-your-code/"},{"title":"RSocket with Spring Boot","text":"How to use RSocket in Spring Boot? RSocket It is a connection-oriented, message-driven protocol with built-in flow control at the application level. It works in a browser equally as well as on a server. In fact, a web browser can serve traffic to backend microservices. It is also binary. It works equally well with text and binary data, and the payloads can be fragmented. It models all the interactions that you do in your application as network primitives. This means you can stream data or do Pub/Sub without having to setup an application queue. – InfoQ In this article, you’ll discover how to do request-response with RSocket using Spring Boot. Step 1: Setup the Server CodeThe Project FileIn the project’s pom.xml file, you need to add the dependency for rsocket. 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-rsocket&lt;/artifactId&gt;&lt;/dependency&gt; The Application PropertiesIn the application.properties file, add the following text: 1spring.rsocket.server.port=7000 The Message Class123456789record Message( @JsonProperty(\"source\") String source, @JsonProperty(\"message\") String message, @JsonProperty(\"createdAt\") long createdAt) { Message { createdAt = Instant.now().getEpochSecond(); }} The Controller Class12345678910@Controller@Slf4jclass RSocketController { @MessageMapping(\"app-socket\") public Message appSocket(Message message) { log.info(\"Received request: {}\", message); return message; }} That’s it for code. Let’s try it. Step 2: Start The Spring Boot RSocket Serverscript1./mvnw clean package spring-boot:run Step 4: Send A Command To The Server With The RSocket CLINext, download RSocket Client CLI and test cli: script123wget -O rsc.jar https://github.com/making/rsc/releases/download/0.5.0/rsc-0.5.0.jarjava -jar rsc.jar --help Next, you’ll send a message to the running server using the RSocket client: script1java -jar rsc.jar --debug --request --data \"{\\\"source\\\":\\\"LeadByExamples\\\",\\\"message\\\":\\\"Hello, RSocket!\\\"}\" --route app-socket tcp://localhost:7000 The Client Output1234567891011121314151617181920212223242526272829302020-08-17 08:14:10.584 DEBUG --- [actor-tcp-nio-1] i.r.FrameLogger : sending -&gt; Frame =&gt; Stream ID: 1 Type: REQUEST_RESPONSE Flags: 0b100000000 Length: 74Metadata: +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f |+--------+-------------------------------------------------+----------------+|00000000| 0a 61 70 70 2d 73 6f 63 6b 65 74 |.app-socket |+--------+-------------------------------------------------+----------------+Data: +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f |+--------+-------------------------------------------------+----------------+|00000000| 7b 22 73 6f 75 72 63 65 22 3a 22 4c 65 61 64 42 |{\"source\":\"LeadB||00000010| 79 45 78 61 6d 70 6c 65 73 22 2c 22 6d 65 73 73 |yExamples\",\"mess||00000020| 61 67 65 22 3a 22 48 65 6c 6c 6f 2c 20 52 53 6f |age\":\"Hello, RSo||00000030| 63 6b 65 74 22 7d |cket\"} |+--------+-------------------------------------------------+----------------+2020-08-17 08:14:10.657 DEBUG --- [actor-tcp-nio-1] i.r.FrameLogger : receiving -&gt; Frame =&gt; Stream ID: 1 Type: NEXT_COMPLETE Flags: 0b1100000 Length: 83Data: +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f |+--------+-------------------------------------------------+----------------+|00000000| 7b 22 73 6f 75 72 63 65 22 3a 22 4c 65 61 64 42 |{\"source\":\"LeadB||00000010| 79 45 78 61 6d 70 6c 65 73 22 2c 22 6d 65 73 73 |yExamples\",\"mess||00000020| 61 67 65 22 3a 22 48 65 6c 6c 6f 2c 20 52 53 6f |age\":\"Hello, RSo||00000030| 63 6b 65 74 22 2c 22 63 72 65 61 74 65 64 41 74 |cket\",\"createdAt||00000040| 22 3a 31 35 39 37 36 33 31 33 35 30 7d |\":1597631350} |+--------+-------------------------------------------------+----------------+{\"source\":\"LeadByExamples\",\"message\":\"Hello, RSocket\",\"createdAt\":1597631350} document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/08/rsocket-with-spring-boot/"},{"title":"JPA Model for Document Structure Entity","text":"JPA Model for Document Structure Entity Thinking in JSON :) Quote of the Day Before software can be reusable it first has to be usable. – Ralph Johnson In SQL, sometimes you want to store directly JSON documents without creating a relational table (like MongoDB or key-value pair). JSON documents support embedded fields, so related data and lists of data can be stored with the document instead of an external table. In this snippet, you’ll find how to map a JSON document structure model in JPA without having a direct relational table. Step 1: Define EntityDocument structure of employee entity that has enum, collection, map, nested object, and nested collection. Step 2: Model entity for JPA1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495enum EmployeeStatus { ACTIVE, FIRED}@Embeddable@Access(AccessType.FIELD)@NoArgsConstructor(access = AccessLevel.PROTECTED)@Getter@AllArgsConstructor@EqualsAndHashCode@ToStringclass Head { private String name; @Embedded private Code code;}@Embeddable@Access(AccessType.FIELD)@NoArgsConstructor(access = AccessLevel.PROTECTED)@Getter@AllArgsConstructor@EqualsAndHashCode@ToStringclass Department { private String name; private Head head; @ElementCollection(fetch = FetchType.EAGER) private Set&lt;Address&gt; addresses; @Embedded private Code code;}@Embeddable@Access(AccessType.FIELD)@NoArgsConstructor(access = AccessLevel.PROTECTED)@Getter@AllArgsConstructor@EqualsAndHashCode@ToStringclass Code { private String code;}@Embeddable@Access(AccessType.FIELD)@NoArgsConstructor(access = AccessLevel.PROTECTED)@Getter@AllArgsConstructor@EqualsAndHashCode@ToStringclass Address { private String addressLine; @Embedded private Code code;}@Entity@Table(name = \"EMPLOYEES\")@Access(AccessType.FIELD)@Getter@NoArgsConstructor(access = AccessLevel.PROTECTED)@EqualsAndHashCode(of = \"id\")@ToString(of = \"id\")class Employee { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String name; @Embedded private Code code; @Embedded private Department department; @ElementCollection(fetch = FetchType.EAGER) private Map&lt;String, Integer&gt; skills; @ElementCollection(fetch = FetchType.EAGER) private Set&lt;Address&gt; addresses; @Enumerated(EnumType.STRING) private EmployeeStatus status; private LocalDate joiningDate; public Employee(String name, Department department, Map&lt;String, Integer&gt; skills, Set&lt;Address&gt; addresses, Code code) { this.name = name; this.department = department; this.skills = skills; this.addresses = addresses; this.joiningDate = LocalDate.now(); this.status = EmployeeStatus.ACTIVE; this.code = code; }} Step 3: JPA Repository12interface EmployeeRepository extends JpaRepository&lt;Employee, Long&gt; {} Step 4: Application PropertiesTo support JPA Multiple Embedded fields with a prefix without having @AttributeOverride annotations: 1spring.jpa.hibernate.naming.implicit-strategy=org.hibernate.boot.model.naming.ImplicitNamingStrategyComponentPathImpl Step 4: Usage1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Spring BootApplication@EnableJpaRepositories(considerNestedRepositories = true)@Slf4jpublic class JPAModelForDocumentStructureEntity { private final EmployeeRepository employeeRepository; public JPAModelForDocumentStructureEntity(EmployeeRepository employeeRepository) { this.employeeRepository = employeeRepository; } @EventListener public void run(ApplicationReadyEvent readyEvent) { Map&lt;String, Integer&gt; skills = new HashMap&lt;&gt;(); skills.put(\"Java\", 90); skills.put(\"Python\", 80); Address addressLine1 = new Address(\"addressLine1\", new Code(\"1-CODE\")); Address addressLine2 = new Address(\"addressLine2\", new Code(\"2-CODE\")); Set&lt;Address&gt; addresses1 = Set.of(addressLine1, addressLine2); Set&lt;Address&gt; addresses2 = Set.of(addressLine1, addressLine2); Head head = new Head(\"h-name\", new Code(\"H-CODE\")); Department department = new Department(\"d-name\", head, addresses1, new Code(\"D-CODE\")); Employee employee = new Employee(\"e-name\", department, skills, addresses2, new Code(\"E-CODE\")); // Save Employee saved = employeeRepository.save(employee); // Find by Id Employee findById = employeeRepository.findById(saved.getId()).get(); log(findById); } private void log(Employee employee) { log.info(\"{}\", employee); log.info(\"-----------------\"); log.info(\"Name: {}\", employee.getName()); log.info(\"Code: {}\", employee.getCode()); log.info(\"Department: {}\", employee.getDepartment()); log.info(\"Skills: {}\", employee.getSkills()); log.info(\"Addresses: {}\", employee.getAddresses()); log.info(\"Status: {}\", employee.getStatus()); log.info(\"Joining Date: {}\", employee.getJoiningDate()); log.info(\"-----------------\"); }} Findings Use Set instead of List to fix the Hibernate MultipleBagFetchException Use field-based access @Access(AccessType.FIELD) strategy why ? Better readability of your code Omit getter or setter methods No need to mark utility methods as @Transient document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/08/jpa-model-for-document-structure-entity/"},{"title":"Graceful shutdown in Spring Boot","text":"Upgrades are inevitable in production with the time, consider you have to relocate currently running service in different clusters which require mostly termination of running application and then move to the target cluster. What will happen with the ongoing requests that are not responded yet by service? If we did a hard shutdown then the server stopped immediately, no response will receive by the client which might provide a bizarre user experience. It’s important to respond to all ongoing requests properly by service before getting killed. Now graceful shutdown comes into play, the service will block the new requests and will wait for ongoing requests to complete. Code is like humor. When you have to explain it, it’s bad.─ Cory House In Spring Boot version 2.3 graceful shutdown is implemented out of the box; when you enabled graceful shutdown, the web server will no longer permit new requests and will wait for a grace period for active requests to complete. Here you can find an example for how to enable a graceful shutdown in spring boot application. Define Controller12345678910@RestController@RequestMapping(\"/orders\")public class OrderController { @GetMapping public List&lt;String&gt; getOrders() throws InterruptedException { TimeUnit.SECONDS.sleep(5); return List.of(\"order-1\", \"order-2\"); }} Enable a graceful shutdown1server.shutdown=graceful The grace period can be configured using spring.lifecycle.timeout-per-shutdown-phase. How to test?Run your application then call API http://localhost:8080/orders, the response will take 5 secs, before 5 secs try to stop running the application. As a result, the server will not stop until your current request is not complete. References https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.3-Release-Notes#graceful-shutdown document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/09/graceful-shutdown-in-spring-boot/"},{"title":"Definition of Ready (DoR) and Definition of Done (DoD)","text":"In software development, the vast majority of the bugs are the consequence of the vague stories and uneven verification of the user acceptance criteria. To lessen such bugs, the idea of “Definition of Ready (DoR)” and “Definition of Done (DoD)” is exercised in a scrum. Furthermore, DoR and DoD are methods for approving work done in agile practice based on rules set by the team for DoR/DoD. Likewise, they are frequently known as an “agile best practice”. Definition of Ready (DoR): Is ready to build? Definition of Ready, or DOR, is a set of criteria that must be satisfied in order for an Epic or a Story to be accepted by the team. The story should be instantly “actionable,” and ready to build. This happens BEFORE you build it, so you can think of this as “pre-validation.” - quora Ok! but what are the quality of a good story? For this widely used method is INVEST acronym in agile, coined by Bill Wake in his article. The good story should be: I – Independent N – Negotiable V – Valuable E – Estimable S – Small T – Testable Photo Credit: agileety.com Definition of Done (DoD) : Is ready to release? Definition of Done, or DOD, is the team’s agreement on what it means to complete a story. This happens AFTER you build it, so you can think of it as “post-validation.” - quora “Done” in an agile practice means “no more work needs to be done before shipping”. Photo Credit: slideshare.net ConclusionDoR/DoD are important concepts in agile development, they help to reduce the number of bugs in production and empower the team with a common checklist to validate their work to consider tasks is really “done for PO (DoR)” and “done for Developers (DoD)”. References https://xp123.com/articles/invest-in-good-stories-and-smart-tasks/ https://www.scrumguides.org/scrum-guide.html#artifact-transparency-done https://www.quora.com/What-are-DOR-and-DOD-in-Scrum document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/09/definition-of-ready-dor-and-definition-of-done-dod/"},{"title":"How to setup elk stack in docker","text":"The Elastic Stack (also known as the ELK Stack) is used across a variety of use cases — from observability to security, from enterprise search to business analytics. ELK is the acronym for three open source projects: Elasticsearch, Logstash, and Kibana. On this blog post, we will go through necessary steps to run elk using docker. Using docker Create a Docker network to enable communication between containers via container name. 1docker network create elk Run elasticsearch docker container. 1docker run -d --name elasticsearch --net elk -p 9200:9200 -e \"discovery.type=single-node\" elasticsearch:7.9.2 Create logstash configuration file logstash.conf. 1234567891011121314cat &lt;&lt;EOF&gt; ~/logstash.confinput { tcp { port =&gt; 5044 codec =&gt; json_lines }}output { elasticsearch { hosts =&gt; [\"http://elasticsearch:9200\"] index =&gt; \"example-%{appname}-%{env}\" }}EOF Run logstash docker container. 1docker run -d --name logstash --net elk -p 5044:5044 -v ~/logstash.conf:/usr/share/logstash/pipeline/logstash.conf logstash:7.9.2 Run kibana docker container.1docker run -d --name kibana --net elk -e \"ELASTICSEARCH_URL=http://elasticsearch:9200\" -p 5601:5601 kibana:7.9.2 Using docker-compose Create logstash configuration file logstash.conf. 1234567891011121314cat &lt;&lt;EOF&gt; ~/logstash.confinput { tcp { port =&gt; 5044 codec =&gt; json_lines }}output { elasticsearch { hosts =&gt; [\"http://elasticsearch:9200\"] index =&gt; \"example-%{appname}-%{env}\" }}EOF docker-compose.yaml configuration for elk stack: 1234567891011121314151617181920212223242526version: \"3.1\"services: elasticsearch: image: elasticsearch:7.9.2 environment: - discovery.type=single-node ports: - 9200 logstash: image: logstash:7.9.2 volumes: - ~/logstash.conf:/usr/share/logstash/pipeline/logstash.conf environment: ELASTICSEARCH_URL: http://elasticsearch:9200 ports: - 5000:5000 links: - elasticsearch kibana: image: kibana:7.9.2 environment: ELASTICSEARCH_URL: http://elasticsearch:9200 ports: - 5601:5601 links: - elasticsearch Command to start elk: 1docker-compose up -d References https://www.elastic.co/what-is/elk-stack document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/10/how-to-setup-elk-stack-in-docker/"},{"title":"Spring Boot App Using Java Code Generated From OpenAPI","text":"How to use OpenAPI to generate code for Spring Boot? You couldn’t find a better place then. I’m going to show you the practical way to generate code and use in spring boot. Over the last three years, my team, and I have been using OpenAPI for API definitions across the microservices. The main benefits of OpenAPI specification: provides a single source for API contracts and easy to render docs by using the same specification. Our team not only benefited from these principle advantages but additionally, we used code generation tools from OpenAPI to generate code for the server application (Spring Boot) as well as for the client application (Angular). Demo &gt; How to do?In this demo, we will see how to build the Spring Boot App using OpenAPI and code generation together. 1. InstallationFirst of all, you must have nodejs and jdk installed on your machine. Then install @openapitools/openapi-generator-cli globally: 1234# install the latest version of \"openapi-generator-cli\"npm install @openapitools/openapi-generator-cli -g# use a specific version of \"openapi-generator-cli\"openapi-generator-cli version-manager set 4.3.1 2. Project SetupBefore writing the code we need to agree on our service API. In this demo, I used the definition that I published in my previous article which specification located in github.com/bhuwanupadhyay/codes. Open the terminal where you want to generate code and run the following command to save the specification. 123wget \\ https://raw.githubusercontent.com/bhuwanupadhyay/codes/main/openapi-docs-using-redoc/dist.yaml \\ -O openapi.yaml I used the following configuration of code generation where you can modify according to your need. The use of each property is described on openapi-generator site. 1234567891011121314151617181920generatorName: 'spring'groupId: 'io.github.bhuwanupadhyay'artifactId: 'spring-boot-using-openapi-code-generation'apiPackage: 'io.github.bhuwanupadhyay.demo.interfaces.rest'modelPackage: 'io.github.bhuwanupadhyay.demo.interfaces.rest.dto'artifactVersion: '1.0.0'library: 'spring-boot'inputSpec: 'openapi.yaml'outputDir: 'demo'additionalProperties: java8: 'true' dateLibrary: 'java8' serializableModel: 'true' serializationLibrary: 'jackson' interfaceOnly: 'true' skipDefaultInterface: 'true' prependFormOrBodyParameters: 'true' useTags: 'true' bigDecimalAsString: 'true' booleanGetterPrefix: 'is' To generate code: 1openapi-generator-cli batch config.yaml The generated minimum structure for java code looks like as below: 1234567rest/┣ dto/┃ ┣ Product.java┃ ┗ User.java┣ ApiUtil.java┣ ProductApi.java┗ UserApi.java 3. API ImplementationThe generated code has skeleton API interfaces on which implementation needs to provide from us. For example, the implementation for Product API Endpoints: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@RestControllerclass WebProductApi implements ProductApi { private final ProductRepository repository; WebProductApi(ProductRepository repository) { this.repository = repository; } @Override public ResponseEntity&lt;Void&gt; createProduct(Product product) { repository.save(toEntity(product)); return ResponseEntity.ok().build(); } @Override public ResponseEntity&lt;Void&gt; deleteProduct(String productId) { return applyVoid(repository, productId, e -&gt; repository.deleteById(e.getProductId())); } @Override public ResponseEntity&lt;Product&gt; getProductById(String productId) { return applyResult(repository, productId, this::toResource); } @Override public ResponseEntity&lt;List&lt;Product&gt;&gt; getProducts(Integer page, Integer size, String sort) { PageRequest request = PageRequest.of(page, size, Sort.by(sort)); Page&lt;ProductEntity&gt; all = repository.findAll(request); List&lt;Product&gt; list = all.stream().map(this::toResource).collect(Collectors.toList()); return ResponseEntity.status(HttpStatus.OK).headers(pageHeaders(all)).body(list); } @Override public ResponseEntity&lt;Void&gt; updateProduct(Product product, String productId) { return applyVoid(repository, productId, e -&gt; { e.with(product.getProductName()); repository.save(e); }); } private Product toResource(ProductEntity entity) { return new Product().productId(entity.getProductId()).productName(entity.getProductName()); } private ProductEntity toEntity(Product product) { ProductEntity entity = new ProductEntity(product.getProductId()); entity.with(product.getProductName()); return entity; } } 4. AdvantagesThere are the following advantages of generated code from the specification. Consistent API contracts. Fail-fast &amp; easy to maintain. Better control of the API interface code. Remove overhead of maintaining POJOs. 5. Source Code &gt; Github LinkConclusionIn summary, the code generation from OpenAPI specification is a very good option to build error-free server code within a short period. I recommend you also give a try and see the results. Reference https://openapi-generator.tech/ https://openapi-generator.tech/docs/generators/spring document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/11/spring-boot-app-using-java-code-generated-from-openapi/"},{"title":"OpenAPI Docs Using ReDoc","text":"How to use ReDoc for OpenAPI? In this blog post, I’m going to show you the practical way to use ReDoc for OpenAPI Documentation. In recent times, Rest APIs are de-facto standards for client-server and service-to-service communication. As a business grows, the number of APIs also increases and, after some time it becomes very hard to manage API without using any comprehensive tool and standard. For this reason, the idea of OpenAPI came into the market and, which is a broadly adopted industry standard for describing modern APIs. The process of defining API is very simple in OpenAPI that just required a single file in json or yaml format, but once the number of API increases then real problems starting to appear in the development and maintenance of this single file. A clever person solves a problem. A wise person AVOIDS it. — Albert Einstein ReDoc allows us to document API into multiple file definitions in standard structure and also provide CLI tool @redocly/openapi-cli that can use for validating definition, to generate docs and export OpenAPI definition. Demo &gt; How to do?In this demo, we will see how to use OpenAPI and ReDoc together for documentation. 1. InstallationFirst of all, you must have nodejs installed on your machine. Then install create-openapi-repo globally: 1npm install -g create-openapi-repo 2. Project SetupRun the following command in your preferred folder to create OpenAPI multiple files and folder structure. 1create-openapi-repo Example: 3. Defining APIsFor example, we have to create services for user management and product management then possible APIs are the following: 1234567|_**User Service**_ | _**Product Service**_ ||----------------------------------------------|----------------------------------------------------|| - POST `/users` to create a new user | - POST `/products` to create a new product || - GET `/users` to get all users | - GET `/products` to get all products || - GET `/users/{username}` to get user | - GET `/products/{productId}` to get product || - PUT `/users/{username}` to edit user | - PUT `/products/{productId}` to edit product || - DELETE `/users/{username}` to delete user | - DELETE `/products/{productId}` to delete product | API definitions using multiple files and folders structure with ReDoc: 12345678910111213141516171819202122232425262728openapi/┣ code_samples/┃ ┣ C#/┃ ┃ ┗ echo/┃ ┃ ┗ post.cs┃ ┣ PHP/┃ ┃ ┗ echo/┃ ┃ ┗ post.php┃ ┗ README.md┣ components/┃ ┣ headers/┃ ┃ ┗ PageInfo.yaml┃ ┣ schemas/┃ ┃ ┣ Email.yaml┃ ┃ ┣ Product.yaml┃ ┃ ┗ User.yaml┃ ┣ securitySchemes/┃ ┃ ┣ api_key.yaml┃ ┃ ┣ basic_auth.yaml┃ ┃ ┗ main_auth.yaml┃ ┗ README.md┣ paths/┃ ┣ products.yaml┃ ┣ products@{productId}.yaml┃ ┣ users.yaml┃ ┗ users@{username}.yaml┣ README.md┗ openapi.yaml In the main openapi yaml file we just need to link our separate definitions with API paths. 123456789paths: '/users': $ref: 'paths/users.yaml' '/users/{username}': $ref: 'paths/users@{username}.yaml' '/products': $ref: 'paths/products.yaml' '/products/{productId}': $ref: 'paths/products@{productId}.yaml' Why multiple files and folders? If you don’t use multiple files and folders structure to manage definitions then you will end with the big file like dist.yaml. After looking at it, you might think it is not bad but this is a demo which has only 8 APIs which may not be in a real-world application. You might have 50 or 100 APIs then you will face the real problem of managing single file so that I recommend multiple files and folder structure to manage API definition. Easy to organize. Easy for development. Easy to re-use of objects to avoid duplication. 4. Lint, Build and Generate Docs To Lint 1npm run test To Build 1npm run build To Generate Docs 1npm run start 5. Source Code &gt; Github LinkConclusionIn summary, managing a single OpenAPI definition file is a daunting task instead by using ReDoc we can organize API development in a better way, and also maintenance becomes very easy. Reference https://github.com/Redocly/redoc https://github.com/Redocly/openapi-cli https://github.com/Redocly/create-openapi-repo document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/11/openapi-docs-using-redoc/"},{"title":"Create and upload custom server certificate in AWS IAM","text":"How to create SSL custom certificate with you domain and upload as server certificate in AWS IAM? Read here… Create setup directory 123mkdir -p ~/customcertscd ~/customcertsrm -rf * Setup variables 123SUBJECT=\"/C=CN/ST=GD/L=SZ/O=Acme, Inc.\"DOMAIN_SUFFIX=example.comCERTIFICATE_NAME=custom-loadbalancer-cert Generate client key &amp; certificate 12openssl genrsa -out ca.key 2048openssl req -new -x509 -days 365 -key ca.key -subj \"$SUBJECT/CN=Acme Root CA\" -out ca.crt Generate server key &amp; certificate 12openssl req -newkey rsa:2048 -nodes -keyout server.key -subj \"$SUBJECT/CN=*.$DOMAIN_SUFFIX\" -out server.csropenssl x509 -req -extfile &lt;(printf \"subjectAltName=DNS:*.$DOMAIN_SUFFIX,DNS:$DOMAIN_SUFFIX,DNS:www.$DOMAIN_SUFFIX\") -days 365 -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt Delete IAM server certificate if exists 1aws iam delete-server-certificate --server-certificate-name $CERTIFICATE_NAME Upload IAM server certificate 1234aws iam upload-server-certificate \\ --server-certificate-name $CERTIFICATE_NAME \\ --certificate-body file://server.crt \\ --private-key file://server.key document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2021/08/create-and-upload-custom-server-certificate-in-aws-iam/"},{"title":"Convert text to speech using Google Text-to-Speech (gTTS) library","text":"How to Convert text to speech using Google Text-to-Speech (gTTS) library? gTTS gTTS (Google Text-to-Speech), a Python library and CLI tool to interface with Google Translate’s &gt; text-to-speech API. Writes spoken mp3 data to a file, a file-like object (bytestring) for further &gt; audio manipulation, or stdout. It features flexible pre-processing and tokenizing. Read More Python: requirements.txt123gTTS==2.2.3playsound==1.3.0PyPDF2==1.26.0 Installation1pip install -r requirements.txt Convert Text to Speech: helloTxt.py12345678910111213141516171819202122# import dependenciesimport osfrom gtts import gTTSfrom playsound import playsoundoutputFile='helloGTTS.mp3'# convert text to speechhelloTxt=\"Hello Everyone! This is Bhuwan Prasad Upadhyay! Welcome to Convert Text To Speech using GTTS Library\"language='en'helloGTTS=gTTS(text=helloTxt,lang=language,slow=False)if os.path.exists(outputFile): os.remove(outputFile)else: print(\"The file does not exist\")# write mp3 file helloGTTS.save(outputFile)# play mp3playsound(outputFile) To run: 1python3 helloTxt.py Convert PDF Text to Speech: pdfTxt.py123456789101112131415161718192021222324252627282930313233# import dependenciesimport ioimport PyPDF2import os from gtts import gTTSfrom playsound import playsoundinputFile='paper.pdf'outputFile='pdfGTTS.mp3'# read pdf as stringpdf = PyPDF2.PdfFileReader(str(inputFile))buf = io.StringIO()for page in pdf.pages: buf.write(page.extractText())# convert text to speechpdfTxt=buf.getvalue()language='en'pdfGTTS=gTTS(text=pdfTxt,lang=language,slow=False)# delete if file exitsif os.path.exists(outputFile): os.remove(outputFile)else: print(\"The file does not exist\")# write mp3 file pdfGTTS.save(outputFile)# play mp3playsound(outputFile) To run: 1python3 pdfTxt.py Github Source document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2021/08/convert-text-to-speech-using-google-text-to-speech-gtts-library/"},{"title":"How to add QR code on pdf using java","text":"How to add QR code on pdf using java? Dependencies123implementation 'com.google.zxing:core:3.4.1'implementation 'com.google.zxing:javase:3.4.1'implementation 'com.itextpdf:itextpdf:5.5.13.2' Add QR Code on PDF using Java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class PdfUtils { /* Add QR code to pdf bytes */ public static byte[] addQRCode(byte[] pdfBytes, String barcodeText, QRCodePosition position) { try (ByteArrayOutputStream os = new ByteArrayOutputStream()) { PdfReader reader = new PdfReader(pdfBytes); PdfStamper stamper = new PdfStamper(reader, os); Image image = Image.getInstance(newQRCodeImage(barcodeText)); for (int i = 1; i &lt;= reader.getNumberOfPages(); i++) { PdfContentByte content = stamper.getOverContent(i); image.setAbsolutePosition(position.getAbsoluteX(), position.getAbsoluteY()); content.addImage(image); } stamper.close(); reader.close(); return os.toByteArray(); } catch (DocumentException | IOException e) { throw new RuntimeException(\"Error on writing QR code\", e); } } /* Get new QR code image bytes for barcode text */ private static byte[] newQRCodeImage(String barcodeText) { try (ByteArrayOutputStream image = new ByteArrayOutputStream()) { QRCodeWriter barcodeWriter = new QRCodeWriter(); BitMatrix bitMatrix = barcodeWriter.encode(barcodeText, BarcodeFormat.QR_CODE, 120, 120); MatrixToImageWriter.writeToStream(bitMatrix, \"png\", image); return image.toByteArray(); } catch (WriterException | IOException e) { throw new RuntimeException(\"Error on generating QR code\", e); } } public enum QRCodePosition { TOP_LEFT(0f, 700f), TOP_RIGHT(500f, 700f), BOTTOM_LEFT(0f, 0f), BOTTOM_RIGHT(500f, 0f); private final float absoluteX; private final float absoluteY; QRCodePosition(float absoluteX, float absoluteY) { this.absoluteX = absoluteX; this.absoluteY = absoluteY; } public float getAbsoluteX() { return absoluteX; } public float getAbsoluteY() { return absoluteY; } }} document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2021/09/how-to-add-qr-code-on-pdf-using-java/"},{"title":"How to convert java object to properties format","text":"How to convert java object to properties format? Dependencies12345&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.dataformat&lt;/groupId&gt; &lt;artifactId&gt;jackson-dataformat-properties&lt;/artifactId&gt; &lt;version&gt;2.12.5&lt;/version&gt;&lt;/dependency&gt; Java Object To Properties Map1234567891011121314151617181920212223242526272829303132333435public class PropsUtils { /** * * Convert java object to properties: all fields, getter methods and is methods into properties map. * * @param object * @param &lt;T&gt; * @return */ public &lt;T&gt; Map&lt;String, String&gt; toProperties(T object) throws IOException { JavaPropsMapper mapper = JavaPropsMapper.builder().build(); JavaPropsSchema javaPropsSchema = JavaPropsSchema.emptySchema().withWriteIndexUsingMarkers(true); return mapper.writeValueAsMap(entity, javaPropsSchema); } /** * * Convert java object to properties: all fields only into properties map. * * @param object * @param &lt;T&gt; * @return */ public &lt;T&gt; Map&lt;String, String&gt; toPropertiesOnlyFields(T object) throws IOException { JavaPropsMapper mapper = JavaPropsMapper.builder() .visibility(PropertyAccessor.FIELD, JsonAutoDetect.Visibility.ANY) .visibility(PropertyAccessor.GETTER, JsonAutoDetect.Visibility.NONE) .visibility(PropertyAccessor.IS_GETTER, JsonAutoDetect.Visibility.NONE).build(); JavaPropsSchema javaPropsSchema = JavaPropsSchema.emptySchema().withWriteIndexUsingMarkers(true); return mapper.writeValueAsMap(entity, javaPropsSchema); } } document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2021/09/how-to-convert-java-object-to-properties-format/"},{"title":"How to split string in bash script","text":"How to split string in bash script? Split string in bash scriptFor example, we want to extract aws region from docker repository url account_id.dkr.ecr.us-east-1.amazonaws.com. 12345docker_url=account_id.dkr.ecr.us-east-1.amazonaws.comIFS='.'#Read the split words into an array based on comma delimiterread -a strarr &lt;&lt;&lt;\"$docker_url\"aws ecr get-login-password --region \"${strarr[3]}\" | docker login --username AWS --password-stdin \"$docker_url\" document.querySelectorAll('.not-gallery-item') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2021/09/how-to-split-string-in-bash-script/"}],"tags":[{"name":"awslambda","slug":"awslambda","link":"/tags/awslambda/"},{"name":"dynamodb","slug":"dynamodb","link":"/tags/dynamodb/"},{"name":"big-decimal","slug":"big-decimal","link":"/tags/big-decimal/"},{"name":"load-testing","slug":"load-testing","link":"/tags/load-testing/"},{"name":"gatling","slug":"gatling","link":"/tags/gatling/"},{"name":"logging","slug":"logging","link":"/tags/logging/"},{"name":"elastic-apm","slug":"elastic-apm","link":"/tags/elastic-apm/"},{"name":"monitoring","slug":"monitoring","link":"/tags/monitoring/"},{"name":"tomcat","slug":"tomcat","link":"/tags/tomcat/"},{"name":"binary-tree","slug":"binary-tree","link":"/tags/binary-tree/"},{"name":"maximum-width","slug":"maximum-width","link":"/tags/maximum-width/"},{"name":"spring-cloud-vault","slug":"spring-cloud-vault","link":"/tags/spring-cloud-vault/"},{"name":"security","slug":"security","link":"/tags/security/"},{"name":"jmh","slug":"jmh","link":"/tags/jmh/"},{"name":"time-complexity","slug":"time-complexity","link":"/tags/time-complexity/"},{"name":"algorithms","slug":"algorithms","link":"/tags/algorithms/"},{"name":"scalable-database-designs","slug":"scalable-database-designs","link":"/tags/scalable-database-designs/"},{"name":"postgres-replication","slug":"postgres-replication","link":"/tags/postgres-replication/"},{"name":"interview","slug":"interview","link":"/tags/interview/"},{"name":"random","slug":"random","link":"/tags/random/"},{"name":"merge-sort","slug":"merge-sort","link":"/tags/merge-sort/"},{"name":"alphanumeric-random","slug":"alphanumeric-random","link":"/tags/alphanumeric-random/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"semantic-release","slug":"semantic-release","link":"/tags/semantic-release/"},{"name":"value-object","slug":"value-object","link":"/tags/value-object/"},{"name":"entity","slug":"entity","link":"/tags/entity/"},{"name":"aggregate-root","slug":"aggregate-root","link":"/tags/aggregate-root/"},{"name":"repository","slug":"repository","link":"/tags/repository/"},{"name":"factory","slug":"factory","link":"/tags/factory/"},{"name":"domain-service","slug":"domain-service","link":"/tags/domain-service/"},{"name":"spring-cloud-stream","slug":"spring-cloud-stream","link":"/tags/spring-cloud-stream/"},{"name":"avro-schema","slug":"avro-schema","link":"/tags/avro-schema/"},{"name":"schema-registry","slug":"schema-registry","link":"/tags/schema-registry/"},{"name":"minikube","slug":"minikube","link":"/tags/minikube/"},{"name":"helm","slug":"helm","link":"/tags/helm/"},{"name":"kubernetes","slug":"kubernetes","link":"/tags/kubernetes/"},{"name":"ingress","slug":"ingress","link":"/tags/ingress/"},{"name":"configmap","slug":"configmap","link":"/tags/configmap/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"semantic-versioning","slug":"semantic-versioning","link":"/tags/semantic-versioning/"},{"name":"containerization","slug":"containerization","link":"/tags/containerization/"},{"name":"ssl","slug":"ssl","link":"/tags/ssl/"},{"name":"wireshark","slug":"wireshark","link":"/tags/wireshark/"},{"name":"java15","slug":"java15","link":"/tags/java15/"},{"name":"sealed-classes","slug":"sealed-classes","link":"/tags/sealed-classes/"},{"name":"local-types","slug":"local-types","link":"/tags/local-types/"},{"name":"rest-template","slug":"rest-template","link":"/tags/rest-template/"},{"name":"error-handler","slug":"error-handler","link":"/tags/error-handler/"},{"name":"architecture-testing","slug":"architecture-testing","link":"/tags/architecture-testing/"},{"name":"rsocket","slug":"rsocket","link":"/tags/rsocket/"},{"name":"json","slug":"json","link":"/tags/json/"},{"name":"document","slug":"document","link":"/tags/document/"},{"name":"mapping","slug":"mapping","link":"/tags/mapping/"},{"name":"graceful-shutdown","slug":"graceful-shutdown","link":"/tags/graceful-shutdown/"},{"name":"dod","slug":"dod","link":"/tags/dod/"},{"name":"dor","slug":"dor","link":"/tags/dor/"},{"name":"elasticsearch","slug":"elasticsearch","link":"/tags/elasticsearch/"},{"name":"kibana","slug":"kibana","link":"/tags/kibana/"},{"name":"logstash","slug":"logstash","link":"/tags/logstash/"},{"name":"spring-boot","slug":"spring-boot","link":"/tags/spring-boot/"},{"name":"openapi","slug":"openapi","link":"/tags/openapi/"},{"name":"redoc","slug":"redoc","link":"/tags/redoc/"},{"name":"aws","slug":"aws","link":"/tags/aws/"},{"name":"iam","slug":"iam","link":"/tags/iam/"},{"name":"server-certificate","slug":"server-certificate","link":"/tags/server-certificate/"},{"name":"text-to-speech","slug":"text-to-speech","link":"/tags/text-to-speech/"},{"name":"gTTS","slug":"gTTS","link":"/tags/gTTS/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"zxing","slug":"zxing","link":"/tags/zxing/"},{"name":"qr-code","slug":"qr-code","link":"/tags/qr-code/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"java-to-props","slug":"java-to-props","link":"/tags/java-to-props/"},{"name":"faster-xml","slug":"faster-xml","link":"/tags/faster-xml/"},{"name":"split-sting-bash-script","slug":"split-sting-bash-script","link":"/tags/split-sting-bash-script/"}],"categories":[{"name":"Serverless","slug":"Serverless","link":"/categories/Serverless/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","link":"/categories/Spring-Boot/"},{"name":"Coding Problems","slug":"Coding-Problems","link":"/categories/Coding-Problems/"},{"name":"Spring Cloud","slug":"Spring-Cloud","link":"/categories/Spring-Cloud/"},{"name":"Microservices","slug":"Microservices","link":"/categories/Microservices/"},{"name":"Semantic Versioning","slug":"Semantic-Versioning","link":"/categories/Semantic-Versioning/"},{"name":"Domain Driven Design","slug":"Domain-Driven-Design","link":"/categories/Domain-Driven-Design/"},{"name":"Minikube","slug":"Minikube","link":"/categories/Minikube/"},{"name":"Java15","slug":"Java15","link":"/categories/Java15/"},{"name":"Spring","slug":"Spring","link":"/categories/Spring/"},{"name":"ArchUnit","slug":"ArchUnit","link":"/categories/ArchUnit/"},{"name":"JPA","slug":"JPA","link":"/categories/JPA/"},{"name":"Agile","slug":"Agile","link":"/categories/Agile/"},{"name":"ELK","slug":"ELK","link":"/categories/ELK/"},{"name":"OpenAPI","slug":"OpenAPI","link":"/categories/OpenAPI/"},{"name":"AWS","slug":"AWS","link":"/categories/AWS/"},{"name":"gTTS","slug":"gTTS","link":"/categories/gTTS/"},{"name":"QR Code","slug":"QR-Code","link":"/categories/QR-Code/"},{"name":"FasterXML","slug":"FasterXML","link":"/categories/FasterXML/"},{"name":"Bash","slug":"Bash","link":"/categories/Bash/"},{"name":"Spring Boot","slug":"OpenAPI/Spring-Boot","link":"/categories/OpenAPI/Spring-Boot/"},{"name":"ReDoc","slug":"OpenAPI/ReDoc","link":"/categories/OpenAPI/ReDoc/"},{"name":"SSL","slug":"AWS/SSL","link":"/categories/AWS/SSL/"}]}